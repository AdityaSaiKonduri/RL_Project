{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10992180,"sourceType":"datasetVersion","datasetId":6841940}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# model and env\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=30, grid_cols=30):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.episode_steps = 0\n        self.goal_team = None\n        self.team_has_ball = -1\n\n    def _get_state(self, player, players):\n        same_team = []\n        other_team = []\n        for play in players:\n            if player.team == play.team:\n                same_team.extend([play.position[0]/(self.grid_rows - 1), \n                                 play.position[1]/(self.grid_cols - 1)])\n            else:\n                other_team.extend([play.position[0]/(self.grid_rows - 1), \n                                 play.position[1]/(self.grid_cols - 1)])\n        if other_team == []:\n            for i in range(len(same_team)//2):\n                other_team.extend([0/(self.grid_rows-1),0/(self.grid_cols - 1)])\n\n        goal_x = self.grid_rows//2 / (self.grid_rows-1)\n        goal_y = (self.grid_cols - 1) / (self.grid_cols - 1) if player.team == 0 else 0\n            \n        return np.array([\n            *same_team,\n            *other_team,\n            self.ball_pos[0]/(self.grid_rows-1),\n            self.ball_pos[1]/(self.grid_cols-1),\n            float(player.has_ball),\n            float(self.team_has_ball),\n            goal_x,\n            goal_y\n        ], dtype=np.float32)\n\n    def reset(self, players, seed=None, options=None, episode_num=0):\n        super().reset(seed=seed)\n        occupied_positions = set()\n        \n        for player in players:\n            player.has_ball = False\n            player.prev_position = None\n            player.moves_with_ball = 0\n            player.moves_without_ball = 0\n            while True:\n                if player.team == 0:\n                    player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n                    player_col = random.randint(1, self.grid_cols//4)\n                else:\n                    player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n                    player_col = random.randint(3*self.grid_cols//4, self.grid_cols-2)\n    \n                if (player_row, player_col) not in occupied_positions:\n                    player.position = (player_row, player_col)\n                    occupied_positions.add((player_row, player_col))\n                    break\n        \n        # Calculate dynamic radius based on episode number\n        # Start with radius 1, increase by 1 every 500 episodes up to grid_cols/4\n        base_radius = 1\n        max_radius = self.grid_cols // 4\n        radius_increase_interval = 500\n        current_radius = min(base_radius + (episode_num // radius_increase_interval), max_radius)\n        \n        # Choose a random player to place the ball near\n        random_player = random.choice(players)\n        \n        max_attempts = 50\n        for _ in range(max_attempts):\n            # Generate a position within current_radius of the player\n            angle = random.uniform(0, 2 * np.pi)\n            r = random.uniform(0, current_radius)\n            delta_row = int(round(r * np.sin(angle)))\n            delta_col = int(round(r * np.cos(angle)))\n            \n            ball_row = random_player.position[0] + delta_row\n            ball_col = random_player.position[1] + delta_col\n            \n            # Ensure ball is within grid bounds and not in occupied positions\n            if (1 <= ball_row < self.grid_rows-1 and \n                1 <= ball_col < self.grid_cols-1 and\n                (ball_row, ball_col) not in occupied_positions):\n                self.ball_pos = (ball_row, ball_col)\n                break\n        else:\n            # Fallback if no valid position found within radius\n            while True:\n                ball_row = random.randint(1, self.grid_rows-2)\n                ball_col = random.randint(self.grid_cols//4, 3*self.grid_cols//4)\n                if (ball_row, ball_col) not in occupied_positions:\n                    self.ball_pos = (ball_row, ball_col)\n                    break\n                \n        self.episode_steps = 0\n        self.goal_team = None\n        self.team_has_ball = -1\n        return {}, {}\n\n    def step(self, action, player, players):\n        self.episode_steps += 1\n        reward = player.step_penalty\n        done = False\n        truncated = False\n        team_reward_applied = False\n    \n        # Store previous ball possession status\n        had_ball_before_move = player.has_ball\n        \n        if action < 8:\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), \n                     (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            new_pos = (player.position[0] + dx, player.position[1] + dy)\n    \n            if (0 <= new_pos[0] < self.grid_rows and \n                0 <= new_pos[1] < self.grid_cols and \n                self.layout[new_pos[0], new_pos[1]] != \"O\"):\n                player.prev_position = player.position\n                player.position = new_pos\n                \n                # Only move ball if player already has it\n                if had_ball_before_move:\n                    self.ball_pos = new_pos\n            else:\n                reward -= 70\n                done = True\n                return self._get_state(player, players), reward, done, True, {}\n    \n        player.prev_ball_statues = player.has_ball\n        player.has_ball = player.position == self.ball_pos\n        if player.has_ball:\n            player.moves_with_ball +=1\n            player.moves_without_ball = 0\n            self.team_has_ball = player.team\n        # elif self.team_has_ball == player.team and had_ball_before_move and not player.has_ball:\n        #     self.team_has_ball = -1\n    \n        if player.has_ball or self.team_has_ball == player.team:\n            reward += (player.ball_possession_bonus * ((0.99) ** (player.moves_with_ball - 1)))\n        else:\n            player.moves_without_ball +=1\n            player.moves_with_ball = 0\n            reward += (player.no_possession * player.moves_without_ball)\n    \n        if not player.has_ball:\n            dist_to_ball = np.sqrt((player.position[0] - self.ball_pos[0])**2 + \n                                  (player.position[1] - self.ball_pos[1])**2)\n            reward += player.near_ball_bonus / (dist_to_ball + 1) \n        \n        if player.has_ball:\n            dist_to_goal = self.grid_cols - 1 - player.position[1]\n            if dist_to_goal < 3:\n                reward += player.near_goal_bonus * (1 - dist_to_goal/10.0)\n    \n        current_ball_cell = self.layout[self.ball_pos[0], self.ball_pos[1]]\n        goal_team = None\n        if current_ball_cell == 'G':\n            goal_team = 0\n        elif current_ball_cell == 'g':\n            goal_team = 1\n    \n        if goal_team is not None and not team_reward_applied:\n            for p in players:\n                if p.team == goal_team:\n                    p_reward = p.goal_reward\n                else:\n                    p_reward = p.opp_scoring\n                \n                if p == player:\n                    reward += p_reward\n                    \n            done = True\n            team_reward_applied = True\n            self.goal_team = goal_team\n    \n        truncated = self.episode_steps >= 3000\n        return self._get_state(player, players), reward, done, truncated, {'goal_team': self.goal_team}\n\n    def render(self, players):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] in ['G', 'g']:\n                    grid[i,j] = '|'\n        \n        grid[self.ball_pos[0], self.ball_pos[1]] = 'B'\n        for player in players:\n            grid[player.position[0], player.position[1]] = 'P' if player.team == 0 else 'Q'\n        \n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n\n\n    def resolve_collisions(self, players):\n        \"\"\"\n        Resolves collisions between players with skill-based probabilities for ball control outcomes.\n        Updates positions and ball possession accordingly.\n        \"\"\"\n        # Dictionary to track positions and players at each position\n        position_map = {}\n        \n        # Group players by position\n        for player in players:\n            position = player.position\n            if position in position_map:\n                position_map[position].append(player)\n            else:\n                position_map[position] = [player]\n        \n        # Track rewards from collisions to return to the players\n        collision_rewards = {player: 0 for player in players}\n        \n        # Process collisions (positions with more than one player)\n        for position, players_at_position in position_map.items():\n            if len(players_at_position) > 1:\n                # Determine ball possession before collision\n                ball_at_position = (self.ball_pos == position)\n                ball_carrier = None\n                \n                # Check if any player at this position has the ball\n                for p in players_at_position:\n                    if p.has_ball:\n                        ball_carrier = p\n                        break\n                \n                # If one player has the ball and there are opponents, simulate tackle/dribble\n                if ball_carrier is not None:\n                    opponents = [p for p in players_at_position if p.team != ball_carrier.team]\n                    \n                    if opponents:\n                        # Handle one-on-one situation\n                        defender = opponents[0] if len(opponents) == 1 else random.choice(opponents)\n                        \n                        # Determine outcome based on skills\n                        # Ball carrier tries to dribble past defender\n                        dribble_success = random.random() < ball_carrier.dribble_success_prob\n                        # Defender tries to tackle ball carrier\n                        tackle_success = random.random() < defender.tackle_success_prob\n                        \n                        # Determine final outcome: tackle succeeds or dribble succeeds\n                        if tackle_success and not dribble_success:\n                            # Defender successfully tackles\n                            previous_carrier = ball_carrier\n                            ball_carrier.has_ball = False\n                            defender.has_ball = True\n                            self.ball_pos = defender.position\n                            self.team_has_ball = defender.team\n                            \n                            # Apply rewards/penalties\n                            collision_rewards[defender] += defender.ball_gained_reward\n                            collision_rewards[previous_carrier] += previous_carrier.ball_lost_penalty\n                            \n                            # Update ball carrier reference\n                            ball_carrier = defender\n                        else:\n                            # Attacker keeps the ball\n                            collision_rewards[ball_carrier] += ball_carrier.dribble_success_prob * 2  # Small reward for successful dribble\n                    \n                # If there was no ball carrier but the ball is at this position,\n                # players try to get the ball based on tackle probabilities\n                elif ball_at_position:\n                    # Group players by team\n                    team_players = {}\n                    for p in players_at_position:\n                        if p.team in team_players:\n                            team_players[p.team].append(p)\n                        else:\n                            team_players[p.team] = [p]\n                    \n                    # If there are players from both teams, they compete for the ball\n                    if len(team_players) > 1:\n                        # Calculate team tackle probabilities (average of all players)\n                        team_tackle_probs = {}\n                        for team, team_list in team_players.items():\n                            team_tackle_probs[team] = sum(p.tackle_success_prob for p in team_list) / len(team_list)\n                        \n                        # Normalize probabilities\n                        total_prob = sum(team_tackle_probs.values())\n                        if total_prob > 0:\n                            for team in team_tackle_probs:\n                                team_tackle_probs[team] /= total_prob\n                        \n                        # Determine which team gets the ball\n                        rand_val = random.random()\n                        cumulative_prob = 0\n                        winning_team = list(team_tackle_probs.keys())[0]  # Default\n                        \n                        for team, prob in team_tackle_probs.items():\n                            cumulative_prob += prob\n                            if rand_val <= cumulative_prob:\n                                winning_team = team\n                                break\n                        \n                        # Choose a random player from the winning team\n                        ball_carrier = random.choice(team_players[winning_team])\n                        ball_carrier.has_ball = True\n                        self.ball_pos = ball_carrier.position\n                        self.team_has_ball = ball_carrier.team\n                        \n                        # Apply rewards\n                        for p in players_at_position:\n                            if p.team == winning_team:\n                                collision_rewards[p] += p.ball_gained_reward * 0.5  # Split the reward among winning team\n                    else:\n                        # Only players from one team, randomly choose one to get the ball\n                        only_team = list(team_players.keys())[0]\n                        ball_carrier = random.choice(team_players[only_team])\n                        ball_carrier.has_ball = True\n                        self.ball_pos = ball_carrier.position\n                        self.team_has_ball = ball_carrier.team\n                \n                # Resolve position conflicts\n                # The ball carrier stays at the position, others move to nearby empty spaces\n                staying_player = ball_carrier if ball_carrier else random.choice(players_at_position)\n                \n                # Move all players except the staying one back to their previous positions\n                for player in players_at_position:\n                    if player != staying_player:\n                        if player.prev_position is not None:\n                            player.position = player.prev_position\n                        else:\n                            # If no previous position, find a nearby empty space\n                            self._find_empty_space(player, players)\n        \n        # Update all players' ball possession status based on final positions\n        for player in players:\n            player.has_ball = (player.position == self.ball_pos)\n            \n            # Update counters for with/without ball\n            if player.has_ball:\n                player.moves_with_ball += 1\n                player.moves_without_ball = 0\n            else:\n                player.moves_without_ball += 1\n                player.moves_with_ball = 0\n        \n        return collision_rewards\n    def _find_empty_space(self, player, all_players):\n        \"\"\"\n        Find an empty adjacent space for a player who needs to be moved.\n        \"\"\"\n        occupied_positions = {p.position for p in all_players}\n        \n        # Check 8 directions around the player\n        possible_moves = [\n            (0, -1), (1, 0), (0, 1), (-1, 0),\n            (-1, -1), (-1, 1), (1, -1), (1, 1)\n        ]\n        \n        for dx, dy in possible_moves:\n            new_pos = (player.position[0] + dx, player.position[1] + dy)\n            \n            # Check if position is valid and empty\n            if (0 <= new_pos[0] < self.grid_rows and \n                0 <= new_pos[1] < self.grid_cols and \n                self.layout[new_pos[0], new_pos[1]] != \"O\" and\n                new_pos not in occupied_positions):\n                player.position = new_pos\n                occupied_positions.add(new_pos)\n                return\n        \n        # If no empty adjacent space, try with a larger radius\n        for radius in range(2, 5):\n            for i in range(-radius, radius + 1):\n                for j in range(-radius, radius + 1):\n                    if i == 0 and j == 0:\n                        continue\n                        \n                    new_pos = (player.position[0] + i, player.position[1] + j)\n                    \n                    if (0 <= new_pos[0] < self.grid_rows and \n                        0 <= new_pos[1] < self.grid_cols and \n                        self.layout[new_pos[0], new_pos[1]] != \"O\" and\n                        new_pos not in occupied_positions):\n                        player.position = new_pos\n                        occupied_positions.add(new_pos)\n                        return\n        \n        # If no position found, keep player at current position and log the issue\n        print(f\"Warning: Could not find empty space for player at {player.position}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:12:50.009624Z","iopub.execute_input":"2025-03-29T12:12:50.010048Z","iopub.status.idle":"2025-03-29T12:12:54.324181Z","shell.execute_reply.started":"2025-03-29T12:12:50.010009Z","shell.execute_reply":"2025-03-29T12:12:54.323427Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# player class\nclass Player:\n    def __init__(self, role, team, env):\n        if role == 'ATK':\n            self.role = role\n            self.team = team\n            self.position = (env.grid_rows//2, env.grid_cols//4 if team == 0 else 3*env.grid_cols//4)\n            self.prev_position = None\n            self.has_ball = False\n            self.goal_reward = 1500\n            self.step_penalty = -0.03\n            self.ball_possession_bonus = 0.2\n            self.near_ball_bonus = 0.001\n            self.near_goal_bonus = 0.002\n            self.opp_scoring = -50\n            self.no_possession = -0.005\n            self.goal_sym = 'G' if team == 0 else 'g'\n            self.opp_goal_sym = 'g' if team == 0 else 'G'\n            self.moves_without_ball = 0\n            self.moves_with_ball = 0\n            self.dribble_success_prob = 0.7 \n            self.tackle_success_prob = 0.3\n            self.ball_gained_reward = 10\n            self.ball_lost_penalty = -5 \n        elif role == 'DF':\n            self.role = role\n            self.team = team\n            self.position = (env.grid_rows//2, env.grid_cols//4 if team == 0 else 3*env.grid_cols//4)\n            self.prev_position = None\n            self.has_ball = False\n            self.goal_reward = 300\n            self.step_penalty = -0.03\n            self.ball_possession_bonus = 0.15\n            self.near_ball_bonus = 0.001\n            self.near_goal_bonus = 0.002\n            self.opp_scoring = -500\n            self.no_possession = -0.002\n            self.goal_sym = 'G' if team == 0 else 'g'\n            self.opp_goal_sym = 'g' if team == 0 else 'G'\n            self.moves_without_ball = 0\n            self.moves_with_ball = 0\n            self.dribble_success_prob = 0.4\n            self.tackle_success_prob = 0.6\n            self.ball_gained_reward = 15\n            self.ball_lost_penalty = -3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:12:54.327782Z","iopub.execute_input":"2025-03-29T12:12:54.328059Z","iopub.status.idle":"2025-03-29T12:12:54.335047Z","shell.execute_reply.started":"2025-03-29T12:12:54.328037Z","shell.execute_reply":"2025-03-29T12:12:54.334204Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# dqn network\nclass Original_DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(Original_DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\nclass DQN(nn.Module):\n    def __init__(self, new_input_dim, new_action_size,old_fc2,old_fc3):\n        super(DQN, self).__init__()\n        print(new_input_dim,new_action_size)\n        self.fc1 = nn.Linear(new_input_dim, 128)\n        self.fc2 = old_fc2\n        self.fc3 = old_fc3\n        self.fc4 = nn.Linear(64, new_action_size)\n\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.zeros_(self.fc1.bias)\n        nn.init.xavier_uniform_(self.fc4.weight)\n        nn.init.zeros_(self.fc4.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size,old_fc2,old_fc3):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.02\n        self.epsilon_decay = 0.99\n        self.learning_rate = 0.0005\n        self.memory = deque(maxlen=200000)\n        self.batch_size = 256\n        self.target_update_freq = 2\n        \n        self.device = device\n        self.model = DQN(state_size, action_size,old_fc2,old_fc3).to(self.device)\n        self.target_model = DQN(state_size, action_size,old_fc2,old_fc3).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        minibatch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor(np.array([t[0] for t in minibatch])).to(self.device)\n        actions = torch.LongTensor([t[1] for t in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(self.device)\n        next_states = torch.FloatTensor(np.array([t[3] for t in minibatch])).to(self.device)\n        dones = torch.FloatTensor([t[4] for t in minibatch]).to(self.device)\n    \n        curr_q = self.model(states).gather(1, actions.unsqueeze(1))\n        next_q = self.target_model(next_states).max(1)[0].detach()\n        target = rewards + (1 - dones) * self.gamma * next_q\n    \n        loss = F.mse_loss(curr_q.squeeze(), target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T12:12:58.605029Z","iopub.execute_input":"2025-03-29T12:12:58.605347Z","iopub.status.idle":"2025-03-29T12:12:58.621109Z","shell.execute_reply.started":"2025-03-29T12:12:58.605321Z","shell.execute_reply":"2025-03-29T12:12:58.620247Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#training\ndef train_multi_agent(players, episodes, max_steps):\n    env = FootballEnv()\n    pre_trained_model_path = \"/kaggle/input/codeee/dqn_football_final.pth\"\n    checkpoint = torch.load(pre_trained_model_path, map_location=torch.device(\"cpu\"))\n    \n    state_dict = checkpoint[\"model_state_dict\"]\n    pre_trained_model = Original_DQN(7, 8)\n    \n    pre_trained_model.load_state_dict(state_dict)\n    pre_trained_model.eval()\n    \n    old_fc2 = pre_trained_model.fc2\n    old_fc3 = pre_trained_model.fc3\n    agents = [DQNAgent(6 + len(players) * 2, 8, old_fc2, old_fc3) for _ in players]\n    \n    for episode in range(episodes):\n        env.reset(players, episode)\n        states = [env._get_state(p, players) for p in players]\n        total_rewards = [0.0 for _ in players]\n        losses = [[] for _ in players]\n        done = False\n        truncated = False\n        goal_team = None\n        \n        for step in range(max_steps):\n            if done or truncated:\n                break\n            \n            actions = [agents[i].act(states[i]) for i in range(len(players))]\n            experiences = [None] * len(players)  # Initialize with placeholders\n            any_player_done = False\n            \n            # First, all players take their actions independently\n            for i in range(len(players)):\n                # Store previous position before taking action\n                players[i].prev_position = players[i].position\n                \n                # Take action but don't check for collisions yet\n                next_state, reward, player_done, player_truncated, info = env.step(\n                    actions[i], players[i], players\n                )\n                \n                # Store the experience\n                experiences[i] = (states[i], actions[i], reward, next_state, player_done or player_truncated)\n                \n                # Update episode termination flags\n                if player_done or player_truncated:\n                    done = player_done\n                    truncated = player_truncated\n                    any_player_done = True\n                \n                # Check for goal\n                if 'goal_team' in info and info['goal_team'] is not None:\n                    goal_team = info['goal_team']\n            \n            # If game isn't done yet, resolve collisions\n            if not any_player_done:\n                env.resolve_collisions(players)\n                \n                # Update states and rewards after collision resolution\n                for i in range(len(players)):\n                    # Get updated state after collisions\n                    updated_state = env._get_state(players[i], players)\n                    \n                    # Calculate additional reward/penalty for collision outcomes\n                    collision_reward = 0\n                    \n                    # Reward for stealing the ball in collision\n                    if not experiences[i][0][6] and players[i].has_ball:  # Wasn't holding ball before but is now\n                        collision_reward += 10\n                    \n                    # Penalty for losing the ball in collision\n                    if experiences[i][0][6] and not players[i].has_ball:  # Was holding ball before but not now\n                        collision_reward -= 10\n                    \n                    # Update the experience with the new state and adjusted reward\n                    old_exp = experiences[i]\n                    updated_reward = old_exp[2] + collision_reward\n                    experiences[i] = (old_exp[0], old_exp[1], updated_reward, updated_state, old_exp[4])\n                    \n                    total_rewards[i] += updated_reward\n                    states[i] = updated_state\n            \n            # Apply goal rewards if a goal was scored\n            if goal_team is not None:\n                for i in range(len(players)):\n                    team_reward = players[i].goal_reward if players[i].team == goal_team else players[i].opp_scoring\n                    old_exp = experiences[i]\n                    updated_reward = old_exp[2] + team_reward\n                    experiences[i] = (old_exp[0], old_exp[1], updated_reward, old_exp[3], True)  # Force done=True for all players\n                    total_rewards[i] += team_reward\n            \n            # Store experiences in replay memory and train\n            for i in range(len(players)):\n                if experiences[i] is not None:  # Make sure experience exists\n                    agents[i].remember(*experiences[i])\n                    if len(agents[i].memory) >= agents[i].batch_size:\n                        loss = agents[i].replay()\n                        losses[i].append(loss)\n                        \n        # Update target models and decay epsilon\n        for idx, agent in enumerate(agents):\n            agent.episode_count += 1\n            if agent.episode_count % agent.target_update_freq == 0:\n                agent.update_target_model()\n            agent.decay_epsilon()\n            agent.rewards_history.append(total_rewards[idx])\n        \n        # Print progress\n        avg_losses = [np.mean(loss) if loss else 0.0 for loss in losses]\n        if episode % 1000 == 0:\n            print(total_rewards)\n            print(f\"Episode: {episode}, Avg Rewards: {np.mean(total_rewards):.2f}, Epsilon: {agents[0].epsilon:.2f}\")\n    \n    return agents\n\nenv = FootballEnv()\nplayers = [Player('ATK', 0, env),Player('DF' , 1 ,env),Player('ATK' , 1 ,env)]\ntrained_agents = train_multi_agent(players, episodes=7001, max_steps=500)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T15:12:54.939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluvating the agents\ndef evaluate_multi_agent(agents, env, players, episodes=1, max_steps=3000, render=True):\n    episode_rewards = []\n    \n    # Print state dimensions for debugging\n    sample_state = env._get_state(players[0], players)\n    print(f\"Evaluation state size: {len(sample_state)}\")\n    print(f\"Agent input size: {agents[0].state_size}\")\n    \n    for episode in range(episodes):\n        env.reset(players, 10000)\n        total_rewards = [0.0 for _ in players]\n        done = False\n        truncated = False\n        goal_team = None\n        \n        for step in range(max_steps):\n            if done or truncated:\n                break\n            \n            if render:\n                env.render(players)\n                time.sleep(0.1)\n            \n            # Get states for all players\n            states = [env._get_state(player, players) for player in players]\n            \n            # Get actions for all players\n            actions = [agents[i].act(states[i], evaluate=True) for i in range(len(players))]\n            any_player_done = False\n            \n            # First, all players take their actions independently\n            for i in range(len(players)):\n                # Store previous position before taking action\n                players[i].prev_position = players[i].position\n                \n                # Take action but don't check for collisions yet\n                next_state, reward, player_done, player_truncated, info = env.step(\n                    actions[i], players[i], players\n                )\n                \n                # Add reward to total\n                total_rewards[i] += reward\n                \n                # Update episode termination flags\n                if player_done or player_truncated:\n                    done = player_done\n                    truncated = player_truncated\n                    any_player_done = True\n                \n                # Check for goal\n                if 'goal_team' in info and info['goal_team'] is not None:\n                    goal_team = info['goal_team']\n            \n            # If game isn't done yet, resolve collisions\n            if not any_player_done:\n                env.resolve_collisions(players)\n                \n                # Update states after collision resolution\n                states = [env._get_state(player, players) for player in players]\n                \n                # Calculate additional rewards/penalties for collision outcomes\n                for i in range(len(players)):\n                    collision_reward = 0\n                    \n                    # These collision rewards should match those in training\n                    # You may need to adjust based on your specific logic\n                    if players[i].has_ball:\n                        collision_reward += 0.5\n                    \n                    total_rewards[i] += collision_reward\n            for i in range(len(players)):\n                print(players[i].position , players[i].has_ball , i , actions[i])\n                # print(players[i].has_ball)\n            \n            # Apply goal rewards if a goal was scored\n            if goal_team is not None:\n                for i in range(len(players)):\n                    team_reward = players[i].goal_reward if players[i].team == goal_team else players[i].opp_scoring\n                    total_rewards[i] += team_reward\n        \n        episode_rewards.append(total_rewards)\n        print(f\"Episode {episode+1} Total Rewards: {total_rewards}\")\n    \n    return np.mean(episode_rewards, axis=0)\n\n\navg_rewards = evaluate_multi_agent(\n    trained_agents,\n    env,\n    players,\n    episodes=1,\n    render=True\n)\nprint(f\"Average rewards across evaluation episodes: {avg_rewards}\")","metadata":{"trusted":true,"scrolled":true,"execution":{"execution_failed":"2025-03-29T15:12:54.940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Hwllo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:07:37.039405Z","iopub.execute_input":"2025-03-29T14:07:37.039675Z","iopub.status.idle":"2025-03-29T14:07:37.052918Z","shell.execute_reply.started":"2025-03-29T14:07:37.039642Z","shell.execute_reply":"2025-03-29T14:07:37.051972Z"}},"outputs":[{"name":"stdout","text":"Hwllo\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}