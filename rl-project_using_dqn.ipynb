{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=50, grid_cols=50):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(7,), dtype=np.float32)\n\n        # Initialize field layout\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n\n        # Initialize positions\n        self.player_pos = (self.grid_rows//2, 5)\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.has_ball = False\n        self.episode_steps = 0\n\n        # Reward parameters\n        self.goal_reward = 50\n        self.step_penalty = -0.001\n        self.ball_possession_bonus = 0.005\n        self.near_ball_bonus = 0.0001\n        self.near_goal_bonus = 0.0002\n\n    def _get_state(self):\n        # Enhanced state representation including has_ball flag\n        return np.array([\n            self.player_pos[0] / (self.grid_rows - 1),\n            self.player_pos[1] / (self.grid_cols - 1),\n            self.ball_pos[0] / (self.grid_rows - 1),\n            self.ball_pos[1] / (self.grid_cols - 1),\n            float(self.has_ball),  # Add has_ball as explicit state feature\n            self.grid_rows // 2 / (self.grid_rows - 1),  # Goal Y position\n            (self.grid_cols - 1) / (self.grid_cols - 1)  # Goal X position\n        ], dtype=np.float32)\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Start with player on left side of field\n        player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n        player_col = random.randint(1, self.grid_cols//4)\n        self.player_pos = (player_row, player_col)\n        \n        # Place ball near player to start\n        ball_row = random.randint(max(1, player_row-3), min(self.grid_rows-2, player_row+3))\n        ball_col = random.randint(max(1, player_col-3), min(self.grid_cols//3, player_col+3))\n        self.ball_pos = (ball_row, ball_col)\n        \n        self.has_ball = (self.player_pos == self.ball_pos)\n        self.episode_steps = 0\n        return self._get_state(), {}\n\n    def step(self, action):\n        self.episode_steps += 1\n        reward = self.step_penalty\n        done = False\n\n        if action < 8:  # Movement actions\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            new_pos = (self.player_pos[0] + dx, self.player_pos[1] + dy)\n\n            if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols and self.layout[new_pos] != \"O\":\n                self.player_pos = new_pos\n                if self.has_ball:\n                    self.ball_pos = new_pos\n            else:\n                reward -= 20  # Massive penalty\n                done = True  # End episode if the player goes out\n                return self._get_state(), reward, done, True, {}\n\n        elif action == 8 and self.has_ball:  # Long shot\n            goal_y_center = self.grid_rows // 2\n            # Better aim toward goal\n            target_y = min(max(goal_y_center + random.randint(-2, 2), 0), self.grid_rows-1)\n            new_ball_col = min(self.ball_pos[1] + 10, self.grid_cols - 1)\n            self.ball_pos = (target_y, new_ball_col)\n            self.has_ball = False\n\n        elif action == 9 and self.has_ball:  # Short pass\n            new_ball_col = min(self.ball_pos[1] + 5, self.grid_cols - 1)\n            self.ball_pos = (self.ball_pos[0], new_ball_col)\n            self.has_ball = False\n\n        # Check if player gets the ball\n        self.has_ball = self.player_pos == self.ball_pos\n        \n        # Reward shaping\n        if self.has_ball:\n            reward += self.ball_possession_bonus\n        \n        # Distance-based rewards\n        dist_to_ball = np.sqrt((self.player_pos[0] - self.ball_pos[0])**2 + \n                              (self.player_pos[1] - self.ball_pos[1])**2)\n        if dist_to_ball < 5 and not self.has_ball:\n            reward += self.near_ball_bonus\n        \n        # Reward for moving toward goal with ball\n        if self.has_ball:\n            # Calculate distance to goal\n            dist_to_goal = self.grid_cols - 1 - self.player_pos[1]\n            if dist_to_goal < 10:\n                reward += self.near_goal_bonus * (1 - dist_to_goal/10.0)\n        \n        # Goal reward\n        if self.layout[self.ball_pos[0], self.ball_pos[1]] == 'G':\n            reward += self.goal_reward\n            done = True\n\n        truncated = self.episode_steps >= 3000  # Shorter episodes\n        return self._get_state(), reward, done, truncated, {}\n\n    def render(self):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        \n        # Draw field elements\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] == 'G':\n                    grid[i,j] = '|'\n                elif self.layout[i,j] == 'M':\n                    grid[i,j] = '.'\n        \n        # Draw player and ball\n        grid[self.player_pos[0], self.player_pos[1]] = 'P'\n        if not self.has_ball:\n            grid[self.ball_pos[0], self.ball_pos[1]] = 'o'\n        \n        # Print the grid\n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n        print(f\"Has ball: {self.has_ball}, Steps: {self.episode_steps}\")\n\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n        \n        # Initialize weights with better defaults\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99  # Discount factor\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.0003\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 256\n        self.target_update_freq = 5  # Update target network every N episodes\n        \n        self.device = device\n        self.model = DQN(state_size, action_size).to(self.device)\n        self.target_model = DQN(state_size, action_size).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        # Sample minibatch from memory\n        minibatch = random.sample(self.memory, self.batch_size)\n        \n        states = torch.FloatTensor([experience[0] for experience in minibatch]).to(self.device)\n        actions = torch.LongTensor([experience[1] for experience in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(self.device)\n        next_states = torch.FloatTensor([experience[3] for experience in minibatch]).to(self.device)\n        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(self.device)\n        \n        # Current Q values\n        curr_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n        \n        # Target Q values\n        with torch.no_grad():\n            next_q_values = self.target_model(next_states).max(1)[0]\n        \n        # Compute target\n        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n        \n        # Compute loss\n        loss = F.mse_loss(curr_q_values.squeeze(), target_q_values)\n        \n        # Backpropagation\n        self.optimizer.zero_grad()\n        loss.backward()\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  \n        self.optimizer.step()\n        \n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def save(self, filepath):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'epsilon': self.epsilon,\n            'episode_count': self.episode_count,\n            'rewards_history': self.rewards_history\n        }, filepath)\n        print(f\"Model saved to {filepath}\")\n\n    def load(self, filepath):\n        checkpoint = torch.load(filepath)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.target_model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.epsilon = checkpoint['epsilon']\n        self.episode_count = checkpoint['episode_count']\n        self.rewards_history = checkpoint['rewards_history']\n        print(f\"Model loaded from {filepath}\")\n\n    def train(self, env, episodes, max_steps=2000, save_freq=50, render_freq=20):\n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            losses = []\n            \n            for step in range(max_steps):\n                action = self.act(state)\n                next_state, reward, done, truncated, _ = env.step(action)\n                total_reward += reward\n                \n                # Store experience in memory\n                self.remember(state, action, reward, next_state, done or truncated)\n                \n                # Train model with replay\n                if len(self.memory) >= self.batch_size:\n                    loss = self.replay()\n                    losses.append(loss)\n                \n                state = next_state\n                \n                if done or truncated:\n                    break\n                    \n            # Update target network periodically\n            if episode % self.target_update_freq == 0:\n                self.update_target_model()\n                \n            # Decay exploration rate\n            self.decay_epsilon()\n            \n            # Record stats\n            self.episode_count += 1\n            self.rewards_history.append(total_reward)\n            \n            # Print episode statistics\n            avg_loss = np.mean(losses) if losses else 0\n            print(f\"Episode {episode}: Reward = {total_reward:.1f}, Steps = {step+1}, Epsilon = {self.epsilon:.3f}, Avg Loss = {avg_loss:.5f}\")\n            \n            # Save the model periodically\n            if episode > 0 and episode % save_freq == 0:\n                self.save(f\"dqn_football_ep{episode}.pth\")\n\n            if episode % 10 == 0:\n                avg_reward = np.mean(self.rewards_history[-10:])\n                print(f\"Last 10 episodes average reward: {avg_reward:.2f}\")\n                \n            # Render occasionally to see progress\n            # if episode % render_freq == 0:\n            #     print(f\"\\n--- Episode {episode} Rendering ---\")\n            #     test_env = FootballEnv()\n            #     self.evaluate(test_env, render=True)\n                \n    def evaluate(self, env, episodes=1, render=True):\n        total_rewards = []\n        \n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            \n            while not done and not truncated:\n                action = self.act(state, evaluate=True)  # No exploration\n                next_state, reward, done, truncated, _ = env.step(action)\n                total_reward += reward\n                \n                if render:\n                    print(action)\n                    env.render()\n                    time.sleep(0.5)  # Pause to make rendering visible\n                    \n                state = next_state\n                \n            total_rewards.append(total_reward)\n            print(f\"Evaluation episode {episode}: Reward = {total_reward}\")\n            \n        return np.mean(total_rewards)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:51:27.006194Z","iopub.execute_input":"2025-02-28T15:51:27.006505Z","iopub.status.idle":"2025-02-28T15:51:27.039751Z","shell.execute_reply.started":"2025-02-28T15:51:27.006481Z","shell.execute_reply":"2025-02-28T15:51:27.038895Z"},"scrolled":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Create environment and agent\nenv = FootballEnv()\nagent = DQNAgent(state_size=7, action_size=10)  # Updated state size\n# Train the agent\nagent.train(env, episodes=5000, save_freq=50)\n# Save the final model\n# agent.save(\"dqn_football_final.pth\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.evaluate(env, episodes=1, render=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}