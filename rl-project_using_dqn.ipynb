{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(7,), dtype=np.float32)\n\n        # Initialize field layout\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n\n        # Initialize positions\n        self.player_pos = (self.grid_rows//2, 5)\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.has_ball = False\n        self.episode_steps = 0\n\n        # Reward parameters\n        self.goal_reward = 50\n        self.step_penalty = -0.001\n        self.ball_possession_bonus = 0.007\n        self.near_ball_bonus = 0.00001\n        self.near_goal_bonus = 0.00002\n\n    def _get_state(self):\n        # Enhanced state representation including has_ball flag\n        return np.array([\n            self.player_pos[0] / (self.grid_rows - 1),\n            self.player_pos[1] / (self.grid_cols - 1),\n            self.ball_pos[0] / (self.grid_rows - 1),\n            self.ball_pos[1] / (self.grid_cols - 1),\n            float(self.has_ball),  # Add has_ball as explicit state feature\n            self.grid_rows // 2 / (self.grid_rows - 1),  # Goal Y position\n            (self.grid_cols - 1) / (self.grid_cols - 1)  # Goal X position\n        ], dtype=np.float32)\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Start with player on left side of field\n        player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n        player_col = random.randint(1, self.grid_cols//4)\n        self.player_pos = (player_row, player_col)\n        \n        # Place ball near player to start\n        ball_row = random.randint(max(1, player_row-3), min(self.grid_rows-2, player_row+3))\n        ball_col = random.randint(max(1, player_col-3), min(self.grid_cols//3, player_col+3))\n        self.ball_pos = (ball_row, ball_col)\n        \n        self.has_ball = (self.player_pos == self.ball_pos)\n        self.episode_steps = 0\n        return self._get_state(), {}\n\n    def step(self, action):\n        self.episode_steps += 1\n        reward = self.step_penalty\n        done = False\n\n        if action < 8:  # Movement actions\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            new_pos = (self.player_pos[0] + dx, self.player_pos[1] + dy)\n\n            if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols and self.layout[new_pos] != \"O\":\n                self.player_pos = new_pos\n                if self.has_ball:\n                    self.ball_pos = new_pos\n            else:\n                reward -= 20  # Massive penalty\n                done = True  # End episode if the player goes out\n                return self._get_state(), reward, done, True, {}\n\n        elif action == 8 and self.has_ball:  # Long shot\n            goal_y_center = self.grid_rows // 2\n            # Better aim toward goal\n            target_y = min(max(goal_y_center + random.randint(-2, 2), 0), self.grid_rows-1)\n            new_ball_col = min(self.ball_pos[1] + 10, self.grid_cols - 1)\n            self.ball_pos = (target_y, new_ball_col)\n            self.has_ball = False\n\n        elif action == 9 and self.has_ball:  # Short pass\n            new_ball_col = min(self.ball_pos[1] + 5, self.grid_cols - 1)\n            self.ball_pos = (self.ball_pos[0], new_ball_col)\n            self.has_ball = False\n\n        # Check if player gets the ball\n        self.has_ball = self.player_pos == self.ball_pos\n        \n        # Reward shaping\n        if self.has_ball:\n            reward += self.ball_possession_bonus\n\n        if self.has_ball and self.player_pos[1] > self.grid_cols//2:\n            reward += 0.001\n        \n        # Distance-based rewards\n        dist_to_ball = np.sqrt((self.player_pos[0] - self.ball_pos[0])**2 + \n                              (self.player_pos[1] - self.ball_pos[1])**2)\n        if dist_to_ball < 5 and not self.has_ball:\n            reward += self.near_ball_bonus\n        \n        # Reward for moving toward goal with ball\n        if self.has_ball:\n            # Calculate distance to goal\n            dist_to_goal = self.grid_cols - 1 - self.player_pos[1]\n            if dist_to_goal < 10:\n                reward += self.near_goal_bonus * (1 - dist_to_goal/10.0)\n        \n        # Goal reward\n        if self.layout[self.ball_pos[0], self.ball_pos[1]] == 'G':\n            reward += self.goal_reward\n            done = True\n\n        truncated = self.episode_steps >= 3000  # Shorter episodes\n        return self._get_state(), reward, done, truncated, {}\n\n    def render(self):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        \n        # Draw field elements\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] == 'G':\n                    grid[i,j] = '|'\n                elif self.layout[i,j] == 'M':\n                    grid[i,j] = '.'\n        \n        # Draw player and ball\n        grid[self.player_pos[0], self.player_pos[1]] = 'P'\n        if not self.has_ball:\n            grid[self.ball_pos[0], self.ball_pos[1]] = 'o'\n        \n        # Print the grid\n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n        print(f\"Has ball: {self.has_ball}, Steps: {self.episode_steps}\")\n\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n        \n        # Initialize weights with better defaults\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99  # Discount factor\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.0003\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 256\n        self.target_update_freq = 5  # Update target network every N episodes\n        \n        self.device = device\n        self.model = DQN(state_size, action_size).to(self.device)\n        self.target_model = DQN(state_size, action_size).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        # Sample minibatch from memory\n        minibatch = random.sample(self.memory, self.batch_size)\n        \n        states = torch.FloatTensor([experience[0] for experience in minibatch]).to(self.device)\n        actions = torch.LongTensor([experience[1] for experience in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(self.device)\n        next_states = torch.FloatTensor([experience[3] for experience in minibatch]).to(self.device)\n        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(self.device)\n        \n        # Current Q values\n        curr_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n        \n        # Target Q values\n        with torch.no_grad():\n            next_q_values = self.target_model(next_states).max(1)[0]\n        \n        # Compute target\n        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n        \n        # Compute loss\n        loss = F.mse_loss(curr_q_values.squeeze(), target_q_values)\n        \n        # Backpropagation\n        self.optimizer.zero_grad()\n        loss.backward()\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  \n        self.optimizer.step()\n        \n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def save(self, filepath):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'epsilon': self.epsilon,\n            'episode_count': self.episode_count,\n            'rewards_history': self.rewards_history\n        }, filepath)\n        print(f\"Model saved to {filepath}\")\n\n    def load(self, filepath):\n        checkpoint = torch.load(filepath)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.target_model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.epsilon = checkpoint['epsilon']\n        self.episode_count = checkpoint['episode_count']\n        self.rewards_history = checkpoint['rewards_history']\n        print(f\"Model loaded from {filepath}\")\n\n    def train(self, env, episodes, max_steps=2000, save_freq=50, render_freq=20):\n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            losses = []\n            \n            for step in range(max_steps):\n                action = self.act(state)\n                next_state, reward, done, truncated, _ = env.step(action)\n                total_reward += reward\n                \n                # Store experience in memory\n                self.remember(state, action, reward, next_state, done or truncated)\n                \n                # Train model with replay\n                if len(self.memory) >= self.batch_size:\n                    loss = self.replay()\n                    losses.append(loss)\n                \n                state = next_state\n                \n                if done or truncated:\n                    break\n                    \n            # Update target network periodically\n            if episode % self.target_update_freq == 0:\n                self.update_target_model()\n                \n            # Decay exploration rate\n            self.decay_epsilon()\n            \n            # Record stats\n            self.episode_count += 1\n            self.rewards_history.append(total_reward)\n            \n            # Print episode statistics\n            avg_loss = np.mean(losses) if losses else 0\n            # print(f\"Episode {episode}: Reward = {total_reward:.1f}, Steps = {step+1}, Epsilon = {self.epsilon:.3f}, Avg Loss = {avg_loss:.5f}\")\n            \n            # Save the model periodically\n            if episode > 0 and episode % save_freq == 0:\n                self.save(f\"dqn_football_ep{episode}.pth\")\n\n            # if episode % 10 == 0:\n            #     avg_reward = np.mean(self.rewards_history[-10:])\n            #     print(f\"Last 10 episodes average reward: {avg_reward:.2f}\")\n                \n            # Render occasionally to see progress\n            # if episode % render_freq == 0:\n            #     print(f\"\\n--- Episode {episode} Rendering ---\")\n            #     test_env = FootballEnv()\n            #     self.evaluate(test_env, render=True)\n                \n    def evaluate(self, env, episodes=1, render=True):\n        total_rewards = []\n        \n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            \n            while not done and not truncated:\n                action = self.act(state, evaluate=True)  # No exploration\n                next_state, reward, done, truncated, _ = env.step(action)\n                total_reward += reward\n                \n                if render:\n                    print(action)\n                    env.render()\n                    time.sleep(0.5)  # Pause to make rendering visible\n                    \n                state = next_state\n                \n            total_rewards.append(total_reward)\n            print(f\"Evaluation episode {episode}: Reward = {total_reward}\")\n            \n        return np.mean(total_rewards)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-02T12:25:36.430171Z","iopub.execute_input":"2025-03-02T12:25:36.430365Z","iopub.status.idle":"2025-03-02T12:25:40.495032Z","shell.execute_reply.started":"2025-03-02T12:25:36.430346Z","shell.execute_reply":"2025-03-02T12:25:40.494376Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Create environment and agent\nenv = FootballEnv()\nagent = DQNAgent(state_size=7, action_size=8)  # Updated state size\n# Train the agent\nagent.train(env, episodes=3000, save_freq=500)\nprint(\"Done\")\n# Save the final model\nagent.save(\"dqn_football_final.pth\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-02T12:25:40.495962Z","iopub.execute_input":"2025-03-02T12:25:40.496260Z","iopub.status.idle":"2025-03-02T12:30:52.738486Z","shell.execute_reply.started":"2025-03-02T12:25:40.496240Z","shell.execute_reply":"2025-03-02T12:30:52.737775Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n<ipython-input-2-c00d90a8a34f>:227: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  states = torch.FloatTensor([experience[0] for experience in minibatch]).to(self.device)\n","output_type":"stream"},{"name":"stdout","text":"Model saved to dqn_football_ep500.pth\nModel saved to dqn_football_ep1000.pth\nModel saved to dqn_football_ep1500.pth\nModel saved to dqn_football_ep2000.pth\nModel saved to dqn_football_ep2500.pth\nDone\nModel saved to dqn_football_final.pth\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"agent.evaluate(env, episodes=1, render=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T12:44:57.892725Z","iopub.execute_input":"2025-03-02T12:44:57.893111Z","iopub.status.idle":"2025-03-02T12:45:03.934495Z","shell.execute_reply.started":"2025-03-02T12:44:57.893080Z","shell.execute_reply":"2025-03-02T12:45:03.933854Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"5\n------------\n|##########|\n|-----.---||\n|-oP--.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: False, Steps: 1\n4\n------------\n|##########|\n|-P---.---||\n|-o---.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: False, Steps: 2\n2\n------------\n|##########|\n|--P--.---||\n|-o---.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: False, Steps: 3\n6\n------------\n|##########|\n|-----.---||\n|-P---.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 4\n2\n------------\n|##########|\n|-----.---||\n|--P--.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 5\n2\n------------\n|##########|\n|-----.---||\n|---P-.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 6\n2\n------------\n|##########|\n|-----.---||\n|----P.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 7\n2\n------------\n|##########|\n|-----.---||\n|-----P---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 8\n2\n------------\n|##########|\n|-----.---||\n|-----.P--||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 9\n2\n------------\n|##########|\n|-----.---||\n|-----.-P-||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 10\n2\n------------\n|##########|\n|-----.---||\n|-----.--P||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 11\n2\n------------\n|##########|\n|-----.---||\n|-----.---P|\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-----.---||\n|-########||\n------------\nHas ball: True, Steps: 12\nEvaluation episode 0: Reward = 50.055138\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"50.055138"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Player:\n    def __init__(self, team, tackle_probability=0.5):\n        \"\"\"\n        Initialize a player with team assignment and tackle capability\n        \n        Args:\n            team: Either 'home' or 'away'\n            tackle_probability: Chance of successful tackle when attempting\n        \"\"\"\n        self.team = team\n        self.tackle_probability = tackle_probability\n        self.position = None\n        self.has_ball = False\n        \n    def reset_position(self, row, col):\n        \"\"\"Set player position on the field\"\"\"\n        self.position = (row, col)\n        \n    def move(self, dx, dy, field_layout, grid_rows, grid_cols):\n        \"\"\"\n        Move player by the given delta if the move is valid\n        \n        Returns:\n            bool: Whether the move was successful\n        \"\"\"\n        new_pos = (self.position[0] + dx, self.position[1] + dy)\n        \n        if (0 <= new_pos[0] < grid_rows and \n            0 <= new_pos[1] < grid_cols and \n            field_layout[new_pos] != \"O\"):\n            self.position = new_pos\n            return True\n        return False\n        \n    def attempt_tackle(self):\n        \"\"\"\n        Attempt to tackle the opponent and get the ball\n        \n        Returns:\n            bool: Whether the tackle was successful\n        \"\"\"\n        return random.random() < self.tackle_probability\n\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        # Action space now includes tackle\n        self.action_space = gym.spaces.Discrete(11)  # 8 movement directions + long shot + short pass + tackle\n        \n        # Observation space includes opponent position\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(9,), dtype=np.float32)\n\n        # Initialize field layout\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n\n        # Initialize players\n        self.player = Player(team='home')\n        self.opponent = Player(team='away')\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.episode_steps = 0\n        \n        # Score tracking\n        self.home_score = 0\n        self.away_score = 0\n\n        # Reward parameters\n        self.goal_reward = 50\n        self.goal_against_penalty = -30\n        self.step_penalty = -0.001\n        self.ball_possession_bonus = 0.007\n        self.near_ball_bonus = 0.00001\n        self.near_goal_bonus = 0.00002\n        self.successful_tackle_bonus = 0.05\n        self.unsuccessful_tackle_penalty = -0.02\n\n    def _get_state(self):\n        # Enhanced state representation including opponent position\n        return np.array([\n            self.player.position[0] / (self.grid_rows - 1),\n            self.player.position[1] / (self.grid_cols - 1),\n            self.opponent.position[0] / (self.grid_rows - 1),\n            self.opponent.position[1] / (self.grid_cols - 1),\n            self.ball_pos[0] / (self.grid_rows - 1),\n            self.ball_pos[1] / (self.grid_cols - 1),\n            float(self.player.has_ball),  # Add has_ball as explicit state feature\n            float(self.opponent.has_ball),  # Add opponent has_ball\n            self.grid_rows // 2 / (self.grid_rows - 1)  # Goal Y position\n        ], dtype=np.float32)\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Start with home player on left side of field\n        player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n        player_col = random.randint(1, self.grid_cols//4)\n        self.player.reset_position(player_row, player_col)\n        \n        # Start with away player on right side of field\n        opponent_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n        opponent_col = random.randint(self.grid_cols*3//4, self.grid_cols-2)\n        self.opponent.reset_position(opponent_row, opponent_col)\n        \n        # Place ball in center\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        \n        # Randomize initial ball possession\n        if random.random() < 0.5:\n            self.player.has_ball = True\n            self.opponent.has_ball = False\n            self.ball_pos = self.player.position\n        else:\n            self.opponent.has_ball = True\n            self.player.has_ball = False\n            self.ball_pos = self.opponent.position\n        \n        self.episode_steps = 0\n        self.home_score = 0\n        self.away_score = 0\n        \n        return self._get_state(), {}\n\n    def _opponent_action(self):\n        \"\"\"Simple AI for the opponent\"\"\"\n        \n        # If opponent has the ball, move toward home goal\n        if self.opponent.has_ball:\n            # Calculate possible moves toward home goal (left side)\n            possible_moves = []\n            \n            # Try to move toward home goal (left side)\n            if self.opponent.position[1] > 0:\n                possible_moves.append((-1, -1))  # Diagonal up-left\n                possible_moves.append((0, -1))   # Left\n                possible_moves.append((1, -1))   # Diagonal down-left\n            \n            # Try vertical moves if in good position horizontally\n            if self.opponent.position[1] < self.grid_cols // 2:\n                if self.opponent.position[0] > self.grid_rows // 2:\n                    possible_moves.append((-1, 0))  # Up\n                else:\n                    possible_moves.append((1, 0))   # Down\n            \n            # If in shooting range, attempt shot\n            if self.opponent.position[1] < 3 and random.random() < 0.7:\n                # Attempt shot at goal\n                target_y = min(max(self.grid_rows // 2 + random.randint(-2, 2), 0), self.grid_rows-1)\n                self.ball_pos = (target_y, 0)\n                self.opponent.has_ball = False\n                return\n            \n            # If no moves available or random choice, try random move\n            if not possible_moves or random.random() < 0.1:\n                possible_moves = [(-1, -1), (-1, 0), (-1, 1), (0, -1), \n                                  (0, 1), (1, -1), (1, 0), (1, 1)]\n            \n            # Try moves until a valid one is found\n            random.shuffle(possible_moves)\n            for dx, dy in possible_moves:\n                if self.opponent.move(dx, dy, self.layout, self.grid_rows, self.grid_cols):\n                    self.ball_pos = self.opponent.position  # Ball moves with opponent\n                    break\n                    \n        # If opponent doesn't have the ball, try to get it\n        else:\n            # If close to player with ball, attempt tackle\n            if (self.player.has_ball and \n                abs(self.opponent.position[0] - self.player.position[0]) <= 1 and\n                abs(self.opponent.position[1] - self.player.position[1]) <= 1):\n                \n                if self.opponent.attempt_tackle():\n                    # Successful tackle\n                    self.opponent.has_ball = True\n                    self.player.has_ball = False\n                    self.ball_pos = self.opponent.position\n                    return\n                    \n            # Move toward the ball\n            dx = 0 if self.opponent.position[0] == self.ball_pos[0] else (\n                  1 if self.opponent.position[0] < self.ball_pos[0] else -1)\n            dy = 0 if self.opponent.position[1] == self.ball_pos[1] else (\n                  1 if self.opponent.position[1] < self.ball_pos[1] else -1)\n            \n            # Try to move toward ball\n            if dx != 0 or dy != 0:\n                self.opponent.move(dx, dy, self.layout, self.grid_rows, self.grid_cols)\n                \n            # Check if opponent got the ball\n            if self.opponent.position == self.ball_pos and not self.player.has_ball:\n                self.opponent.has_ball = True\n\n    def step(self, action):\n        self.episode_steps += 1\n        reward = self.step_penalty\n        done = False\n        info = {'scored': False, 'conceded': False}\n\n        # Process player action\n        if action < 8:  # Movement actions\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            \n            if self.player.move(dx, dy, self.layout, self.grid_rows, self.grid_cols):\n                if self.player.has_ball:\n                    self.ball_pos = self.player.position\n            else:\n                reward -= 0.1  # Small penalty for invalid move attempt\n        \n        elif action == 8 and self.player.has_ball:  # Long shot\n            goal_y_center = self.grid_rows // 2\n            # Better aim toward goal\n            target_y = min(max(goal_y_center + random.randint(-2, 2), 0), self.grid_rows-1)\n            new_ball_col = min(self.ball_pos[1] + 10, self.grid_cols - 1)\n            self.ball_pos = (target_y, new_ball_col)\n            self.player.has_ball = False\n\n        elif action == 9 and self.player.has_ball:  # Short pass\n            new_ball_col = min(self.ball_pos[1] + 5, self.grid_cols - 1)\n            self.ball_pos = (self.ball_pos[0], new_ball_col)\n            self.player.has_ball = False\n            \n        elif action == 10:  # Tackle\n            # Check if player is near opponent with ball\n            if (self.opponent.has_ball and \n                abs(self.player.position[0] - self.opponent.position[0]) <= 1 and\n                abs(self.player.position[1] - self.opponent.position[1]) <= 1):\n                \n                if self.player.attempt_tackle():\n                    # Successful tackle\n                    self.player.has_ball = True\n                    self.opponent.has_ball = False\n                    self.ball_pos = self.player.position\n                    reward += self.successful_tackle_bonus\n                else:\n                    # Failed tackle\n                    reward += self.unsuccessful_tackle_penalty\n\n        # Check if player gets the ball (if it's free)\n        if self.player.position == self.ball_pos and not self.opponent.has_ball:\n            self.player.has_ball = True\n        \n        # Process opponent action\n        self._opponent_action()\n        \n        # Reward shaping\n        if self.player.has_ball:\n            reward += self.ball_possession_bonus\n\n        if self.player.has_ball and self.player.position[1] > self.grid_cols//2:\n            reward += 0.001\n        \n        # Distance-based rewards\n        if not self.player.has_ball and not self.opponent.has_ball:\n            dist_to_ball = np.sqrt((self.player.position[0] - self.ball_pos[0])**2 + \n                                  (self.player.position[1] - self.ball_pos[1])**2)\n            if dist_to_ball < 5:\n                reward += self.near_ball_bonus\n        \n        # Reward for moving toward goal with ball\n        if self.player.has_ball:\n            # Calculate distance to goal\n            dist_to_goal = self.grid_cols - 1 - self.player.position[1]\n            if dist_to_goal < 10:\n                reward += self.near_goal_bonus * (1 - dist_to_goal/10.0)\n        \n        # Check for goals\n        # Home team scores (player)\n        if self.layout[self.ball_pos[0], self.ball_pos[1]] == 'G':\n            reward += self.goal_reward\n            self.home_score += 1\n            info['scored'] = True\n            # Reset positions but keep score\n            self.reset()\n            \n        # Away team scores (opponent)\n        elif self.layout[self.ball_pos[0], self.ball_pos[1]] == 'g':\n            reward += self.goal_against_penalty\n            self.away_score += 1\n            info['conceded'] = True\n            # Reset positions but keep score\n            self.reset()\n            \n        # End game if either team scores 3 goals\n        if self.home_score >= 3 or self.away_score >= 3:\n            done = True\n\n        truncated = self.episode_steps >= 2000  # Shorter episodes\n        \n        # Add score to info\n        info['home_score'] = self.home_score\n        info['away_score'] = self.away_score\n        \n        return self._get_state(), reward, done, truncated, info\n\n    def render(self):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        \n        # Draw field elements\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] == 'G':\n                    grid[i,j] = '|'\n                elif self.layout[i,j] == 'g':\n                    grid[i,j] = '|'\n                elif self.layout[i,j] == 'M':\n                    grid[i,j] = '.'\n        \n        # Draw player, opponent and ball\n        grid[self.player.position[0], self.player.position[1]] = 'P'\n        grid[self.opponent.position[0], self.opponent.position[1]] = 'E'\n        \n        # Draw ball if not possessed\n        if not self.player.has_ball and not self.opponent.has_ball:\n            grid[self.ball_pos[0], self.ball_pos[1]] = 'o'\n        \n        # Print the grid\n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n        print(f\"Score: Home {self.home_score} - {self.away_score} Away\")\n        print(f\"Player has ball: {self.player.has_ball}, Opponent has ball: {self.opponent.has_ball}\")\n        print(f\"Steps: {self.episode_steps}\")\n\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n        \n        # Initialize weights with better defaults\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99  # Discount factor\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.0003\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 256\n        self.target_update_freq = 5  # Update target network every N episodes\n        \n        self.device = device\n        self.model = DQN(state_size, action_size).to(self.device)\n        self.target_model = DQN(state_size, action_size).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n        self.goals_scored = 0\n        self.goals_conceded = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        # Sample minibatch from memory\n        minibatch = random.sample(self.memory, self.batch_size)\n        \n        states = torch.FloatTensor([experience[0] for experience in minibatch]).to(self.device)\n        actions = torch.LongTensor([experience[1] for experience in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(self.device)\n        next_states = torch.FloatTensor([experience[3] for experience in minibatch]).to(self.device)\n        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(self.device)\n        \n        # Current Q values\n        curr_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n        \n        # Target Q values\n        with torch.no_grad():\n            next_q_values = self.target_model(next_states).max(1)[0]\n        \n        # Compute target\n        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n        \n        # Compute loss\n        loss = F.mse_loss(curr_q_values.squeeze(), target_q_values)\n        \n        # Backpropagation\n        self.optimizer.zero_grad()\n        loss.backward()\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  \n        self.optimizer.step()\n        \n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def save(self, filepath):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'epsilon': self.epsilon,\n            'episode_count': self.episode_count,\n            'rewards_history': self.rewards_history,\n            'goals_scored': self.goals_scored,\n            'goals_conceded': self.goals_conceded\n        }, filepath)\n        print(f\"Model saved to {filepath}\")\n\n    def load(self, filepath):\n        checkpoint = torch.load(filepath)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.target_model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.epsilon = checkpoint['epsilon']\n        self.episode_count = checkpoint['episode_count']\n        self.rewards_history = checkpoint['rewards_history']\n        self.goals_scored = checkpoint.get('goals_scored', 0)\n        self.goals_conceded = checkpoint.get('goals_conceded', 0)\n        print(f\"Model loaded from {filepath}\")\n\n    def train(self, env, episodes, max_steps=2000, save_freq=50, render_freq=20):\n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            losses = []\n            \n            for step in range(max_steps):\n                action = self.act(state)\n                next_state, reward, done, truncated, info = env.step(action)\n                total_reward += reward\n                \n                # Store experience in memory\n                self.remember(state, action, reward, next_state, done or truncated)\n                \n                # Track goals\n                if info.get('scored', False):\n                    self.goals_scored += 1\n                if info.get('conceded', False):\n                    self.goals_conceded += 1\n                \n                # Train model with replay\n                if len(self.memory) >= self.batch_size:\n                    loss = self.replay()\n                    losses.append(loss)\n                \n                state = next_state\n                \n                if done or truncated:\n                    break\n                    \n            # Update target network periodically\n            if episode % self.target_update_freq == 0:\n                self.update_target_model()\n                \n            # Decay exploration rate\n            self.decay_epsilon()\n            \n            # Record stats\n            self.episode_count += 1\n            self.rewards_history.append(total_reward)\n            \n            # Print episode statistics\n            avg_loss = np.mean(losses) if losses else 0\n            if episode % 10 == 0:\n                print(f\"Episode {episode}: Reward = {total_reward:.1f}, Steps = {step+1}, Epsilon = {self.epsilon:.3f}, Avg Loss = {avg_loss:.5f}\")\n                print(f\"Goals: {self.goals_scored} scored, {self.goals_conceded} conceded\")\n            \n            # # Save the model periodically\n            # if episode > 0 and episode % save_freq == 0:\n            #     self.save(f\"dqn_football_ep{episode}.pth\")\n\n            # if episode % 10 == 0:\n            #     avg_reward = np.mean(self.rewards_history[-10:])\n            #     print(f\"Last 10 episodes average reward: {avg_reward:.2f}\")\n                \n            # # Render occasionally to see progress\n            # if episode % render_freq == 0:\n            #     print(f\"\\n--- Episode {episode} Rendering ---\")\n            #     test_env = FootballEnv()\n            #     self.evaluate(test_env, render=True)\n                \n    def evaluate(self, env, episodes=1, render=True):\n        total_rewards = []\n        goals_scored = 0\n        goals_conceded = 0\n        \n        for episode in range(episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            done = False\n            truncated = False\n            \n            while not done and not truncated:\n                action = self.act(state, evaluate=True)  # No exploration\n                next_state, reward, done, truncated, info = env.step(action)\n                total_reward += reward\n                \n                # Track goals during evaluation\n                if info.get('scored', False):\n                    goals_scored += 1\n                if info.get('conceded', False):\n                    goals_conceded += 1\n                \n                if render:\n                    print(f\"Action: {action}\")\n                    env.render()\n                    time.sleep(0.5)  # Pause to make rendering visible\n                    \n                state = next_state\n                \n            total_rewards.append(total_reward)\n            print(f\"Evaluation episode {episode}: Reward = {total_reward}\")\n            \n        print(f\"Evaluation goals: {goals_scored} scored, {goals_conceded} conceded\")\n        return np.mean(total_rewards)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create environment and agent\n    env = FootballEnv()\n    agent = DQNAgent(state_size=9, action_size=11)  # Updated state size and action size\n    # Train the agent\n    agent.train(env, episodes=3000, save_freq=500)\n    # Save the final model\n    agent.save(\"dqn_football_final.pth\")\n    print(\"Training complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:18:53.408405Z","iopub.execute_input":"2025-03-01T10:18:53.408752Z"}},"outputs":[{"name":"stdout","text":"Episode 0: Reward = -5695.9, Steps = 2000, Epsilon = 0.995, Avg Loss = 53.67758\nGoals: 43 scored, 261 conceded\nEpisode 10: Reward = -3764.8, Steps = 2000, Epsilon = 0.946, Avg Loss = 275.91888\nGoals: 628 scored, 2630 conceded\nEpisode 20: Reward = -4267.5, Steps = 2000, Epsilon = 0.900, Avg Loss = 384.31072\nGoals: 1296 scored, 4994 conceded\nEpisode 30: Reward = -3210.7, Steps = 2000, Epsilon = 0.856, Avg Loss = 440.78407\nGoals: 2017 scored, 7284 conceded\nEpisode 40: Reward = -2308.1, Steps = 2000, Epsilon = 0.814, Avg Loss = 351.68972\nGoals: 2801 scored, 9539 conceded\nEpisode 50: Reward = -659.3, Steps = 2000, Epsilon = 0.774, Avg Loss = 279.67012\nGoals: 3950 scored, 11724 conceded\nEpisode 60: Reward = -156.1, Steps = 2000, Epsilon = 0.737, Avg Loss = 320.72654\nGoals: 5178 scored, 13885 conceded\nEpisode 70: Reward = 1525.1, Steps = 2000, Epsilon = 0.701, Avg Loss = 360.78508\nGoals: 6521 scored, 15968 conceded\nEpisode 80: Reward = 1586.8, Steps = 2000, Epsilon = 0.666, Avg Loss = 333.31462\nGoals: 7883 scored, 18016 conceded\nEpisode 90: Reward = 3225.6, Steps = 2000, Epsilon = 0.634, Avg Loss = 367.97255\nGoals: 9506 scored, 19993 conceded\nEpisode 100: Reward = 2897.1, Steps = 2000, Epsilon = 0.603, Avg Loss = 363.57825\nGoals: 11200 scored, 21925 conceded\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Player:\n    def __init__(self, steal_prob=0.3):\n        self.pos = (0, 0)\n        self.has_ball = False\n        self.steal_prob = steal_prob\n        self.team = None  # 'A' or 'B' for different teams\n\n    def reset(self, pos, team):\n        self.pos = pos\n        self.has_ball = False\n        self.team = team\n\n    def attempt_steal(self, other):\n        if random.random() < self.steal_prob:\n            if other.has_ball:\n                other.has_ball = False\n                self.has_ball = True\n                return True\n        return False\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(9,), dtype=np.float32)\n\n        # Initialize field layout\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self._initialize_layout()\n        \n        # Initialize players and ball\n        self.player = Player(steal_prob=0.4)\n        self.opponent = Player(steal_prob=0.35)\n        self.ball_pos = (grid_rows//2, grid_cols//2)\n        self.episode_steps = 0\n\n        # Reward parameters\n        self.goal_reward = 50\n        self.step_penalty = -0.01\n        self.ball_possession_bonus = 0.1\n        self.near_ball_bonus = 0.001\n        self.concede_penalty = -30\n\n    def _initialize_layout(self):\n        self.layout[:, :] = \".\"\n        mid_row, mid_col = self.grid_rows//2, self.grid_cols//2\n        \n        # Goals and center\n        self.layout[mid_row-4:mid_row+5, -1] = \"G\"\n        self.layout[mid_row-4:mid_row+5, 0] = \"g\"\n        self.layout[:, mid_col] = \"M\"\n        \n        # Field boundaries\n        self.layout[[0, -1], :] = \"O\"\n        self.layout[:, [0, -1]] = \"O\"\n        self.layout[0, mid_col-4:mid_col+5] = \".\"\n        self.layout[-1, mid_col-4:mid_col+5] = \".\"\n\n    def _get_state(self, agent):\n        if agent == 'player':\n            player = self.player\n            opponent = self.opponent\n            goal_x = self.grid_cols - 1\n        else:\n            player = self.opponent\n            opponent = self.player\n            goal_x = 0\n\n        return np.array([\n            player.pos[0] / (self.grid_rows-1),\n            player.pos[1] / (self.grid_cols-1),\n            -opponent.pos[0]/(self.grid_rows-1),  # Negative for opponent\n            -opponent.pos[1]/(self.grid_cols-1),\n            self.ball_pos[0]/(self.grid_rows-1),\n            self.ball_pos[1]/(self.grid_cols-1),\n            float(player.has_ball),\n            goal_x/(self.grid_cols-1),\n            (self.grid_rows//2)/(self.grid_rows-1)\n        ], dtype=np.float32)\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Initialize positions\n        self.player.reset(\n            (random.randint(2, self.grid_rows-3), \n            random.randint(2, self.grid_cols//4)),\n            'A'\n        )\n        self.opponent.reset(\n            (random.randint(2, self.grid_rows-3), \n            random.randint(3*self.grid_cols//4, self.grid_cols-3)),\n            'B'\n        )\n        \n        # Place ball near player\n        self.ball_pos = (\n            random.randint(max(1, self.player.pos[0]-2), \n            min(self.grid_rows-2, self.player.pos[0]+2)),\n            random.randint(max(1, self.player.pos[1]-2),\n            min(self.grid_cols//3, self.player.pos[1]+2))\n        )\n        self.player.has_ball = (self.player.pos == self.ball_pos)\n        self.opponent.has_ball = False\n        self.episode_steps = 0\n        \n        return self._get_state('player'), {}\n\n    def _move_agent(self, agent, action):\n        dx, dy = 0, 0\n        if action < 8:\n            moves = [(0,-1),(1,0),(0,1),(-1,0),\n                    (-1,-1),(-1,1),(1,-1),(1,1)]\n            dx, dy = moves[action]\n        \n        new_pos = (agent.pos[0]+dx, agent.pos[1]+dy)\n        if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols:\n            if self.layout[new_pos] != 'O':\n                agent.pos = new_pos\n                if agent.has_ball:\n                    self.ball_pos = new_pos\n                return True\n        return False\n\n    def step(self, action):\n        self.episode_steps += 1\n        p_reward = o_reward = self.step_penalty\n        done = truncated = False\n\n        # Player action\n        player_valid = self._move_agent(self.player, action)\n        if not player_valid:\n            p_reward -= 2.0\n\n        # Opponent action (random policy for initial training)\n        opponent_action = random.randint(0, 9)\n        opponent_valid = self._move_agent(self.opponent, opponent_action)\n        if not opponent_valid:\n            o_reward -= 2.0\n\n        # Ball handling\n        ball_carrier = None\n        if self.player.has_ball:\n            ball_carrier = self.player\n        elif self.opponent.has_ball:\n            ball_carrier = self.opponent\n\n        # Check ball possession\n        if self.player.pos == self.ball_pos and self.opponent.pos == self.ball_pos:\n            if self.player.attempt_steal(self.opponent):\n                p_reward += 0.5\n                o_reward -= 0.5\n            elif self.opponent.attempt_steal(self.player):\n                o_reward += 0.5\n                p_reward -= 0.5\n        else:\n            self.player.has_ball = (self.player.pos == self.ball_pos)\n            self.opponent.has_ball = (self.opponent.pos == self.ball_pos)\n\n        # Shooting/passing\n        if action == 8 and self.player.has_ball:  # Long shot\n            self.ball_pos = (self.ball_pos[0], min(self.ball_pos[1]+8, self.grid_cols-1))\n            self.player.has_ball = False\n        elif action == 9 and self.player.has_ball:  # Short pass\n            self.ball_pos = (self.ball_pos[0], min(self.ball_pos[1]+4, self.grid_cols-1))\n            self.player.has_ball = False\n\n        # Goal scoring\n        if self.layout[self.ball_pos] == 'G':\n            p_reward += self.goal_reward\n            o_reward += self.concede_penalty\n            done = True\n        elif self.layout[self.ball_pos] == 'g':\n            o_reward += self.goal_reward\n            p_reward += self.concede_penalty\n            done = True\n\n        # Additional rewards\n        if self.player.has_ball:\n            p_reward += self.ball_possession_bonus\n            p_reward += (self.player.pos[1] / self.grid_cols) * 0.01  # Forward progress\n        if self.opponent.has_ball:\n            o_reward += self.ball_possession_bonus\n\n        # Distance-based rewards\n        p_dist = abs(self.player.pos[0]-self.ball_pos[0]) + abs(self.player.pos[1]-self.ball_pos[1])\n        o_dist = abs(self.opponent.pos[0]-self.ball_pos[0]) + abs(self.opponent.pos[1]-self.ball_pos[1])\n        p_reward += self.near_ball_bonus / (p_dist + 1)\n        o_reward += self.near_ball_bonus / (o_dist + 1)\n\n        truncated = self.episode_steps >= 1500\n        return (\n            self._get_state('player'), \n            {'player_reward': p_reward, 'opponent_reward': o_reward},\n            done,\n            truncated,\n            {}\n        )\n\n    def render(self):\n        grid = np.full((self.grid_rows, self.grid_cols), ' ')\n        \n        # Draw field\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O': grid[i,j] = '#'\n                elif self.layout[i,j] in ['G','g']: grid[i,j] = '|'\n        \n        # Draw actors\n        grid[self.ball_pos] = 'o'\n        grid[self.player.pos] = 'P' if self.player.has_ball else 'p'\n        grid[self.opponent.pos] = 'O' if self.opponent.has_ball else 'o'\n        \n        # Print\n        print('\\n' + '='*30)\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print(f\"Steps: {self.episode_steps}\")\n\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_size))\n        \n        for layer in self.net:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_normal_(layer.weight)\n                nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass MultiAgentDQN:\n    def __init__(self, state_size, action_size):\n        self.agents = {\n            'player': DQNAgent(state_size, action_size),\n            'opponent': DQNAgent(state_size, action_size)\n        }\n\n    def train(self, env, episodes):\n        for ep in range(episodes):\n            state, _ = env.reset()\n            total_rewards = {'player': 0, 'opponent': 0}\n            \n            while True:\n                # Get actions from both agents\n                p_action = self.agents['player'].act(state)\n                next_state, rewards, done, truncated, _ = env.step(p_action)\n                \n                # Opponent acts based on its own state\n                o_state = env._get_state('opponent')\n                o_action = self.agents['opponent'].act(o_state)\n                next_o_state = env._get_state('opponent')\n                \n                # Store experiences\n                self.agents['player'].remember(\n                    state, p_action, rewards['player_reward'], \n                    next_state, done or truncated\n                )\n                self.agents['opponent'].remember(\n                    o_state, o_action, rewards['opponent_reward'],\n                    next_o_state, done or truncated\n                )\n                \n                # Train both agents\n                p_loss = self.agents['player'].replay()\n                o_loss = self.agents['opponent'].replay()\n                \n                # Update tracking\n                total_rewards['player'] += rewards['player_reward']\n                total_rewards['opponent'] += rewards['opponent_reward']\n                state = next_state\n                \n                if done or truncated:\n                    break\n                \n            # Post-episode updates\n            for agent in self.agents.values():\n                agent.decay_epsilon()\n                agent.update_target_model()\n                \n            if ep % 50 == 0:\n                print(f\"Episode {ep}: Player R={total_rewards['player']:.1f}, Opponent R={total_rewards['opponent']:.1f}\")\n                self.agents['player'].save(f\"player_ep{ep}.pth\")\n                self.agents['opponent'].save(f\"opponent_ep{ep}.pth\")\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=100000)\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.997\n        self.batch_size = 128\n        self.model = DQN(state_size, action_size).to(device)\n        self.target_model = DQN(state_size, action_size).to(device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0002)\n        self.update_freq = 5\n\n    def act(self, state):\n        if random.random() < self.epsilon:\n            return random.randint(0, self.action_size-1)\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        return self.model(state).argmax().item()\n\n    def remember(self, *args):\n        self.memory.append(args)\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        curr_q = self.model(states).gather(1, actions.unsqueeze(1))\n        next_q = self.target_model(next_states).max(1)[0].detach()\n        target = rewards + (1 - dones) * self.gamma * next_q\n        \n        loss = F.mse_loss(curr_q.squeeze(), target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n        self.optimizer.step()\n        \n        return loss.item()\n\n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def save(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load(self, path):\n        self.model.load_state_dict(torch.load(path))\n        self.target_model.load_state_dict(self.model.state_dict())\n\n\n# Training setup\nif __name__ == \"__main__\":\n    env = FootballEnv(grid_rows=12, grid_cols=16)\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n\n    \n    multi_agent = MultiAgentDQN(state_size, action_size)\n    multi_agent.train(env, episodes=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:44:42.913749Z","iopub.execute_input":"2025-03-02T13:44:42.914110Z","iopub.status.idle":"2025-03-02T14:00:35.678962Z","shell.execute_reply.started":"2025-03-02T13:44:42.914080Z","shell.execute_reply":"2025-03-02T14:00:35.676452Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-ad31c3e7f9dc>:345: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  states = torch.FloatTensor(states).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Episode 0: Player R=-287.8, Opponent R=-298.5\nEpisode 50: Player R=-293.6, Opponent R=-322.9\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ad31c3e7f9dc>\u001b[0m in \u001b[0;36m<cell line: 378>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mmulti_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiAgentDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mmulti_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-ad31c3e7f9dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;31m# Train both agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opponent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0;31m# Update tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-ad31c3e7f9dc>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mcurr_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mnext_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-ad31c3e7f9dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"print(\"Hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:00:40.338406Z","iopub.execute_input":"2025-03-02T14:00:40.338724Z","iopub.status.idle":"2025-03-02T14:00:40.343400Z","shell.execute_reply.started":"2025-03-02T14:00:40.338697Z","shell.execute_reply":"2025-03-02T14:00:40.342261Z"}},"outputs":[{"name":"stdout","text":"Hello\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Player:\n    def __init__(self, steal_prob=0.3):\n        self.pos = (0, 0)\n        self.has_ball = False\n        self.steal_prob = steal_prob\n        self.team = None  # 'A' or 'B' for different teams\n\n    def reset(self, pos, team):\n        self.pos = pos\n        self.has_ball = False\n        self.team = team\n\n    def attempt_steal(self, other):\n        if random.random() < self.steal_prob:\n            if other.has_ball:\n                other.has_ball = False\n                self.has_ball = True\n                return True\n        return False\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(9,), dtype=np.float32)\n\n        # Initialize field layout\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self._initialize_layout()\n        \n        # Initialize players and ball\n        self.player = Player(steal_prob=0.4)\n        self.opponent = Player(steal_prob=0.35)\n        self.ball_pos = (grid_rows//2, grid_cols//2)\n        self.episode_steps = 0\n\n        # Reward parameters\n        self.goal_reward = 50\n        self.step_penalty = -0.01\n        self.ball_possession_bonus = 0.1\n        self.near_ball_bonus = 0.001\n        self.concede_penalty = -30\n\n    def _initialize_layout(self):\n        self.layout[:, :] = \".\"\n        mid_row, mid_col = self.grid_rows//2, self.grid_cols//2\n        \n        # Goals and center\n        self.layout[mid_row-4:mid_row+5, -1] = \"G\"\n        self.layout[mid_row-4:mid_row+5, 0] = \"g\"\n        self.layout[:, mid_col] = \"M\"\n        \n        # Field boundaries\n        self.layout[[0, -1], :] = \"O\"\n        self.layout[:, [0, -1]] = \"O\"\n        self.layout[0, mid_col-4:mid_col+5] = \".\"\n        self.layout[-1, mid_col-4:mid_col+5] = \".\"\n\n    def _get_state(self, agent):\n        if agent == 'player':\n            player = self.player\n            opponent = self.opponent\n            goal_x = self.grid_cols - 1\n        else:\n            player = self.opponent\n            opponent = self.player\n            goal_x = 0\n\n        return np.array([\n            player.pos[0] / (self.grid_rows-1),\n            player.pos[1] / (self.grid_cols-1),\n            -opponent.pos[0]/(self.grid_rows-1),  # Negative for opponent\n            -opponent.pos[1]/(self.grid_cols-1),\n            self.ball_pos[0]/(self.grid_rows-1),\n            self.ball_pos[1]/(self.grid_cols-1),\n            float(player.has_ball),\n            goal_x/(self.grid_cols-1),\n            (self.grid_rows//2)/(self.grid_rows-1)\n        ], dtype=np.float32)\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Initialize positions\n        self.player.reset(\n            (random.randint(2, self.grid_rows-3), \n            random.randint(2, self.grid_cols//4)),\n            'A'\n        )\n        self.opponent.reset(\n            (random.randint(2, self.grid_rows-3), \n            random.randint(3*self.grid_cols//4, self.grid_cols-3)),\n            'B'\n        )\n        \n        # Place ball near player\n        self.ball_pos = (\n            random.randint(max(1, self.player.pos[0]-2), \n            min(self.grid_rows-2, self.player.pos[0]+2)),\n            random.randint(max(1, self.player.pos[1]-2),\n            min(self.grid_cols//3, self.player.pos[1]+2))\n        )\n        self.player.has_ball = (self.player.pos == self.ball_pos)\n        self.opponent.has_ball = False\n        self.episode_steps = 0\n        \n        return self._get_state('player'), {}\n\n    def _move_agent(self, agent, action):\n        dx, dy = 0, 0\n        if action < 8:\n            moves = [(0,-1),(1,0),(0,1),(-1,0),\n                    (-1,-1),(-1,1),(1,-1),(1,1)]\n            dx, dy = moves[action]\n        \n        new_pos = (agent.pos[0]+dx, agent.pos[1]+dy)\n        if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols:\n            if self.layout[new_pos] != 'O':\n                agent.pos = new_pos\n                if agent.has_ball:\n                    self.ball_pos = new_pos\n                return True\n        return False\n\n    def step(self, action):\n        self.episode_steps += 1\n        p_reward = o_reward = self.step_penalty\n        done = truncated = False\n\n        # Player action\n        player_valid = self._move_agent(self.player, action)\n        if not player_valid:\n            p_reward -= 2.0\n\n        # Opponent action (random policy for initial training)\n        opponent_action = random.randint(0, 9)\n        opponent_valid = self._move_agent(self.opponent, opponent_action)\n        if not opponent_valid:\n            o_reward -= 2.0\n\n        # Ball handling\n        ball_carrier = None\n        if self.player.has_ball:\n            ball_carrier = self.player\n        elif self.opponent.has_ball:\n            ball_carrier = self.opponent\n\n        # Check ball possession\n        if self.player.pos == self.ball_pos and self.opponent.pos == self.ball_pos:\n            if self.player.attempt_steal(self.opponent):\n                p_reward += 0.5\n                o_reward -= 0.5\n            elif self.opponent.attempt_steal(self.player):\n                o_reward += 0.5\n                p_reward -= 0.5\n        else:\n            self.player.has_ball = (self.player.pos == self.ball_pos)\n            self.opponent.has_ball = (self.opponent.pos == self.ball_pos)\n\n        # Shooting/passing\n        if action == 8 and self.player.has_ball:  # Long shot\n            self.ball_pos = (self.ball_pos[0], min(self.ball_pos[1]+8, self.grid_cols-1))\n            self.player.has_ball = False\n        elif action == 9 and self.player.has_ball:  # Short pass\n            self.ball_pos = (self.ball_pos[0], min(self.ball_pos[1]+4, self.grid_cols-1))\n            self.player.has_ball = False\n\n        # Goal scoring\n        if self.layout[self.ball_pos] == 'G':\n            p_reward += self.goal_reward\n            o_reward += self.concede_penalty\n            done = True\n        elif self.layout[self.ball_pos] == 'g':\n            o_reward += self.goal_reward\n            p_reward += self.concede_penalty\n            done = True\n\n        # Additional rewards\n        if self.player.has_ball:\n            p_reward += self.ball_possession_bonus\n            p_reward += (self.player.pos[1] / self.grid_cols) * 0.01  # Forward progress\n        if self.opponent.has_ball:\n            o_reward += self.ball_possession_bonus\n\n        # Distance-based rewards\n        p_dist = abs(self.player.pos[0]-self.ball_pos[0]) + abs(self.player.pos[1]-self.ball_pos[1])\n        o_dist = abs(self.opponent.pos[0]-self.ball_pos[0]) + abs(self.opponent.pos[1]-self.ball_pos[1])\n        p_reward += self.near_ball_bonus / (p_dist + 1)\n        o_reward += self.near_ball_bonus / (o_dist + 1)\n\n        truncated = self.episode_steps >= 1500\n        return (\n            self._get_state('player'), \n            {'player_reward': p_reward, 'opponent_reward': o_reward},\n            done,\n            truncated,\n            {}\n        )\n\n    def render(self):\n        grid = np.full((self.grid_rows, self.grid_cols), ' ')\n        \n        # Draw field\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O': grid[i,j] = '#'\n                elif self.layout[i,j] in ['G','g']: grid[i,j] = '|'\n        \n        # Draw actors\n        grid[self.ball_pos] = 'o'\n        grid[self.player.pos] = 'P' if self.player.has_ball else 'p'\n        grid[self.opponent.pos] = 'O' if self.opponent.has_ball else 'o'\n        \n        # Print\n        print('\\n' + '='*30)\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print(f\"Steps: {self.episode_steps}\")\n\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_size))\n        \n        for layer in self.net:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_normal_(layer.weight)\n                nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass MultiAgentDQN:\n    def __init__(self, state_size, action_size):\n        self.agents = {\n            'player': DQNAgent(state_size, action_size),\n            'opponent': DQNAgent(state_size, action_size)\n        }\n        self.train_scores = {'player': [], 'opponent': []}\n        \n    def train(self, env, episodes, render_every=100):\n        for ep in range(episodes):\n            state, _ = env.reset()\n            total_rewards = {'player': 0, 'opponent': 0}\n            \n            while True:\n                # Get actions from both agents\n                p_action = self.agents['player'].act(state)\n                next_state, rewards, done, truncated, _ = env.step(p_action)\n                \n                # Opponent acts based on its own state\n                o_state = env._get_state('opponent')\n                o_action = self.agents['opponent'].act(o_state)\n                next_o_state = env._get_state('opponent')\n                \n                # Store experiences\n                self.agents['player'].remember(\n                    state, p_action, rewards['player_reward'], \n                    next_state, done or truncated\n                )\n                self.agents['opponent'].remember(\n                    o_state, o_action, rewards['opponent_reward'],\n                    next_o_state, done or truncated\n                )\n                \n                # Train both agents\n                p_loss = self.agents['player'].replay()\n                o_loss = self.agents['opponent'].replay()\n                \n                # Update tracking\n                total_rewards['player'] += rewards['player_reward']\n                total_rewards['opponent'] += rewards['opponent_reward']\n                state = next_state\n                \n                # Render if requested\n                # if ep % render_every == 0:\n                #     env.render()\n                #     time.sleep(0.1)  # Slow down rendering\n                \n                if done or truncated:\n                    break\n                \n            # Post-episode updates\n            for agent in self.agents.values():\n                agent.decay_epsilon()\n                agent.update_target_model()\n                \n            # Track scores\n            self.train_scores['player'].append(total_rewards['player'])\n            self.train_scores['opponent'].append(total_rewards['opponent'])\n                \n            if ep % 10 == 0:\n                print(f\"Episode {ep}: Player R={total_rewards['player']:.1f}, Opponent R={total_rewards['opponent']:.1f}\")\n                self.agents['player'].save(f\"player_ep{ep}.pth\")\n                self.agents['opponent'].save(f\"opponent_ep{ep}.pth\")\n\n    def evaluate(self, env, episodes=5, render=True):\n        \"\"\"\n        Evaluate trained agents without exploration\n        \"\"\"\n        total_rewards = {'player': 0, 'opponent': 0}\n        goals_scored = {'player': 0, 'opponent': 0}\n        \n        for ep in range(episodes):\n            state, _ = env.reset()\n            ep_rewards = {'player': 0, 'opponent': 0}\n            done = truncated = False\n            \n            print(f\"\\n==== EVALUATION EPISODE {ep+1} ====\")\n            \n            step = 0\n            while not (done or truncated):\n                # Get action without exploration\n                p_action = self.agents['player'].act_eval(state)\n                next_state, rewards, done, truncated, _ = env.step(p_action)\n                \n                # Update rewards\n                ep_rewards['player'] += rewards['player_reward']\n                ep_rewards['opponent'] += rewards['opponent_reward']\n                state = next_state\n                \n                # Render\n                if render and step % 5 == 0:  # Render every 5 steps to speed up visualization\n                    env.render()\n                    time.sleep(0.2)  # Slow down rendering\n                \n                step += 1\n            \n            # Track goals\n            if done:\n                if ep_rewards['player'] > 0:\n                    goals_scored['player'] += 1\n                else:\n                    goals_scored['opponent'] += 1\n            \n            # Final render\n            if render:\n                env.render()\n            \n            # Track total rewards\n            total_rewards['player'] += ep_rewards['player']\n            total_rewards['opponent'] += ep_rewards['opponent']\n            \n            print(f\"Episode {ep+1} Results:\")\n            print(f\"  Player reward: {ep_rewards['player']:.1f}\")\n            print(f\"  Opponent reward: {ep_rewards['opponent']:.1f}\")\n            print(f\"  Steps: {step}\")\n            \n        # Summary\n        avg_p_reward = total_rewards['player'] / episodes\n        avg_o_reward = total_rewards['opponent'] / episodes\n        \n        print(\"\\n==== EVALUATION SUMMARY ====\")\n        print(f\"Episodes: {episodes}\")\n        print(f\"Player avg reward: {avg_p_reward:.1f}\")\n        print(f\"Opponent avg reward: {avg_o_reward:.1f}\")\n        print(f\"Player goals: {goals_scored['player']}\")\n        print(f\"Opponent goals: {goals_scored['opponent']}\")\n        \n        return avg_p_reward, avg_o_reward, goals_scored\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=100000)\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.997\n        self.batch_size = 128\n        self.model = DQN(state_size, action_size).to(device)\n        self.target_model = DQN(state_size, action_size).to(device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0002)\n        self.update_freq = 5\n\n    def act(self, state):\n        if random.random() < self.epsilon:\n            return random.randint(0, self.action_size-1)\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        return self.model(state).argmax().item()\n    \n    def act_eval(self, state):\n        \"\"\"Act without exploration for evaluation\"\"\"\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        return self.model(state).argmax().item()\n\n    def remember(self, *args):\n        self.memory.append(args)\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n        \n        # Sample batch\n        batch = random.sample(self.memory, self.batch_size)\n        \n        # Convert batch of tuples to tuple of lists\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        # Convert lists to numpy arrays first (fixing the warning)\n        states = np.array(states)\n        actions = np.array(actions)\n        rewards = np.array(rewards)\n        next_states = np.array(next_states)\n        dones = np.array(dones)\n        \n        # Convert numpy arrays to tensors\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        curr_q = self.model(states).gather(1, actions.unsqueeze(1))\n        next_q = self.target_model(next_states).max(1)[0].detach()\n        target = rewards + (1 - dones) * self.gamma * next_q\n        \n        loss = F.mse_loss(curr_q.squeeze(), target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n        self.optimizer.step()\n        \n        return loss.item()\n\n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def save(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load(self, path):\n        self.model.load_state_dict(torch.load(path))\n        self.target_model.load_state_dict(self.model.state_dict())\n\n\n# Training setup with evaluation\nif __name__ == \"__main__\":\n    env = FootballEnv(grid_rows=12, grid_cols=16)\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n\n    \n    multi_agent = MultiAgentDQN(state_size, action_size)\n    \n    # Training with rendering every 100 episodes\n    multi_agent.train(env, episodes=1000, render_every=1001)\n    \n    # Final evaluation after training\n    print(\"\\nRunning final evaluation...\")\n    multi_agent.evaluate(env, episodes=1, render=True)\n    \n    # Plot learning curves\n    try:\n        import matplotlib.pyplot as plt\n        \n        # Moving average to smooth the curves\n        def moving_average(data, window_size=10):\n            return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n        \n        # Plot rewards\n        plt.figure(figsize=(12, 6))\n        \n        # Get moving averages\n        window = 20\n        if len(multi_agent.train_scores['player']) > window:\n            p_ma = moving_average(multi_agent.train_scores['player'], window)\n            o_ma = moving_average(multi_agent.train_scores['opponent'], window)\n            \n            plt.plot(range(window-1, len(p_ma)+window-1), p_ma, label='Player')\n            plt.plot(range(window-1, len(o_ma)+window-1), o_ma, label='Opponent')\n            plt.xlabel('Episode')\n            plt.ylabel('Reward')\n            plt.title('Training Rewards (Moving Average)')\n            plt.legend()\n            plt.grid(True)\n            plt.savefig('football_training_rewards.png')\n            plt.show()\n            \n    except ImportError:\n        print(\"Matplotlib not available. Skipping reward plots.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:01:54.109334Z","iopub.execute_input":"2025-03-02T14:01:54.109637Z","iopub.status.idle":"2025-03-02T15:26:03.962034Z","shell.execute_reply.started":"2025-03-02T14:01:54.109616Z","shell.execute_reply":"2025-03-02T15:26:03.960689Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Episode 0: Player R=-258.2, Opponent R=-182.8\nEpisode 10: Player R=-269.6, Opponent R=-335.5\nEpisode 20: Player R=-270.5, Opponent R=-192.0\nEpisode 30: Player R=-255.3, Opponent R=-302.7\nEpisode 40: Player R=-233.0, Opponent R=-230.7\nEpisode 50: Player R=-179.3, Opponent R=-260.2\nEpisode 60: Player R=-299.2, Opponent R=-248.7\nEpisode 70: Player R=-328.1, Opponent R=-312.9\nEpisode 80: Player R=-363.1, Opponent R=-270.8\nEpisode 90: Player R=-236.7, Opponent R=-404.9\nEpisode 100: Player R=-355.9, Opponent R=-310.8\nEpisode 110: Player R=-280.7, Opponent R=-237.2\nEpisode 120: Player R=-330.7, Opponent R=-382.8\nEpisode 130: Player R=-402.1, Opponent R=-280.7\nEpisode 140: Player R=-269.7, Opponent R=-262.9\nEpisode 150: Player R=-303.7, Opponent R=-264.8\nEpisode 160: Player R=-272.6, Opponent R=-262.9\nEpisode 170: Player R=-440.7, Opponent R=-236.8\nEpisode 180: Player R=-262.9, Opponent R=-382.8\nEpisode 190: Player R=-269.0, Opponent R=-268.8\nEpisode 200: Player R=-275.5, Opponent R=-296.9\nEpisode 210: Player R=-297.4, Opponent R=-198.8\nEpisode 220: Player R=-449.4, Opponent R=-182.8\nEpisode 230: Player R=-318.0, Opponent R=-292.8\nEpisode 240: Player R=-340.2, Opponent R=-304.9\nEpisode 250: Player R=-195.5, Opponent R=-318.8\nEpisode 260: Player R=-324.9, Opponent R=-332.8\nEpisode 270: Player R=-315.2, Opponent R=-259.3\nEpisode 280: Player R=-162.7, Opponent R=-181.3\nEpisode 290: Player R=-246.9, Opponent R=-356.9\nEpisode 300: Player R=-237.4, Opponent R=-388.8\nEpisode 310: Player R=-228.7, Opponent R=-270.9\nEpisode 320: Player R=-122.3, Opponent R=-288.8\nEpisode 330: Player R=-212.0, Opponent R=-297.6\nEpisode 340: Player R=-351.5, Opponent R=-242.3\nEpisode 350: Player R=-160.4, Opponent R=-328.0\nEpisode 360: Player R=-222.0, Opponent R=-236.9\nEpisode 370: Player R=-129.3, Opponent R=-306.8\nEpisode 380: Player R=-191.9, Opponent R=-396.9\nEpisode 390: Player R=-287.3, Opponent R=-328.9\nEpisode 400: Player R=-201.7, Opponent R=-240.9\nEpisode 410: Player R=-127.0, Opponent R=-398.8\nEpisode 420: Player R=-90.9, Opponent R=-178.8\nEpisode 430: Player R=-125.1, Opponent R=-290.2\nEpisode 440: Player R=-220.7, Opponent R=-345.5\nEpisode 450: Player R=-102.4, Opponent R=-360.8\nEpisode 460: Player R=-122.0, Opponent R=-226.8\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-52e05d1d2096>\u001b[0m in \u001b[0;36m<cell line: 469>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;31m# Training with rendering every 100 episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0mmulti_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;31m# Final evaluation after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-52e05d1d2096>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, episodes, render_every)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# Train both agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opponent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-52e05d1d2096>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;34m\"set `error_if_nonfinite=False`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}