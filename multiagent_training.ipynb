{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# model and env\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.episode_steps = 0\n        self.goal_team = None\n        self.team_has_ball = -1\n\n    def _get_state(self, player, players):\n        same_team = []\n        other_team = []\n        for play in players:\n            if player.team == play.team:\n                same_team.extend([play.position[0]/self.grid_rows - 1, \n                                 play.position[1]/self.grid_cols - 1])\n            else:\n                other_team.extend([play.position[0]/self.grid_rows - 1, \n                                 play.position[1]/self.grid_cols - 1])\n        other_team = [-1*x for x in other_team]\n\n        goal_x = self.grid_rows//2 / (self.grid_rows-1)\n        goal_y = (self.grid_cols - 1) / (self.grid_cols - 1) if player.team == 0 else 0\n            \n        return np.array([\n            *same_team,\n            *other_team,\n            float(player.has_ball),\n            float(self.team_has_ball),\n            goal_x,\n            goal_y\n        ], dtype=np.float32)\n\n    def reset(self, players, seed=None, options=None):\n        super().reset(seed=seed)\n        occupied_positions = set()\n        \n        for player in players:\n            player.has_ball = False\n            player.prev_position = None\n            while True:\n                if player.team == 0:\n                    player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n                    player_col = random.randint(1, self.grid_cols//4)\n                else:\n                    player_row = random.randint(self.grid_rows//2-3, self.grid_rows//2+3)\n                    player_col = random.randint(3*self.grid_cols//4, self.grid_cols-2)\n\n                if (player_row, player_col) not in occupied_positions:\n                    player.position = (player_row, player_col)\n                    occupied_positions.add((player_row, player_col))\n                    break\n\n        while True:\n            ball_row = random.randint(1, self.grid_rows-2)\n            ball_col = random.randint(self.grid_cols//4, 3*self.grid_cols//4)\n            if (ball_row, ball_col) not in occupied_positions:\n                self.ball_pos = (ball_row, ball_col)\n                break\n                \n        self.episode_steps = 0\n        self.goal_team = None\n        return {}, {}\n\n    def step(self, action, player, players):\n        self.episode_steps += 1\n        reward = player.step_penalty\n        done = False\n        truncated = False\n        team_reward_applied = False\n    \n        # Store previous ball possession status\n        had_ball_before_move = player.has_ball\n        \n        if action < 8:\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), \n                     (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            new_pos = (player.position[0] + dx, player.position[1] + dy)\n    \n            if (0 <= new_pos[0] < self.grid_rows and \n                0 <= new_pos[1] < self.grid_cols and \n                self.layout[new_pos[0], new_pos[1]] != \"O\"):\n                player.prev_position = player.position\n                player.position = new_pos\n                \n                # Only move ball if player already has it\n                if had_ball_before_move:\n                    self.ball_pos = new_pos\n            else:\n                reward -= 20\n                done = True\n                return self._get_state(player, players), reward, done, True, {}\n    \n        # Update ball possession status AFTER movement\n        player.prev_ball_statues = player.has_ball  # Keeping your original typo for consistency\n        player.has_ball = player.position == self.ball_pos\n        # print(player.has_ball)\n        \n        # Update which team has the ball\n        if player.has_ball:\n            self.team_has_ball = player.team\n        elif self.team_has_ball == player.team and had_ball_before_move and not player.has_ball:\n            # If this player's team had the ball (through this player) and doesn't anymore\n            self.team_has_ball = -1\n    \n        if player.has_ball:\n            reward += player.ball_possession_bonus + 0.001\n    \n        if not player.has_ball and self.team_has_ball != player.team:\n            reward += player.no_possession\n    \n        dist_to_ball = np.sqrt((player.position[0] - self.ball_pos[0])**2 + \n                              (player.position[1] - self.ball_pos[1])**2)\n        if dist_to_ball < 5 and not player.has_ball:\n            reward += player.near_ball_bonus\n        \n        if player.has_ball:\n            dist_to_goal = self.grid_cols - 1 - player.position[1]\n            if dist_to_goal < 10:\n                reward += player.near_goal_bonus * (1 - dist_to_goal/10.0)\n    \n        current_ball_cell = self.layout[self.ball_pos[0], self.ball_pos[1]]\n        goal_team = None\n        if current_ball_cell == 'G':\n            goal_team = 0\n        elif current_ball_cell == 'g':\n            goal_team = 1\n    \n        if goal_team is not None and not team_reward_applied:\n            for p in players:\n                if p.team == goal_team:\n                    p_reward = p.goal_reward\n                else:\n                    p_reward = p.opp_scoring\n                \n                if p == player:\n                    reward += p_reward\n                    \n            done = True\n            team_reward_applied = True\n            self.goal_team = goal_team\n    \n        truncated = self.episode_steps >= 3000\n        return self._get_state(player, players), reward, done, truncated, {'goal_team': self.goal_team}\n\n    def render(self, players):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] in ['G', 'g']:\n                    grid[i,j] = '|'\n        \n        grid[self.ball_pos[0], self.ball_pos[1]] = 'B'\n        for player in players:\n            grid[player.position[0], player.position[1]] = 'P' if player.team == 0 else 'Q'\n        \n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n\nclass Player:\n    def __init__(self, role, team, env):\n        self.role = role\n        self.team = team\n        self.position = (env.grid_rows//2, env.grid_cols//4 if team == 0 else 3*env.grid_cols//4)\n        self.prev_position = None\n        self.has_ball = False\n        self.goal_reward = 52\n        self.step_penalty = -0.003\n        self.ball_possession_bonus = 0.008\n        self.near_ball_bonus = 0.00001\n        self.near_goal_bonus = 0.00002\n        self.opp_scoring = -50\n        self.no_possession = -0.0005\n        self.goal_sym = 'G' if team == 0 else 'g'\n        self.opp_goal_sym = 'g' if team == 0 else 'G'\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n        \n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.99995\n        self.learning_rate = 0.0003\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 256\n        self.target_update_freq = 5\n        \n        self.device = device\n        self.model = DQN(state_size, action_size).to(self.device)\n        self.target_model = DQN(state_size, action_size).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        minibatch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor(np.array([t[0] for t in minibatch])).to(self.device)\n        actions = torch.LongTensor([t[1] for t in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(self.device)\n        next_states = torch.FloatTensor(np.array([t[3] for t in minibatch])).to(self.device)\n        dones = torch.FloatTensor([t[4] for t in minibatch]).to(self.device)\n    \n        curr_q = self.model(states).gather(1, actions.unsqueeze(1))\n        next_q = self.target_model(next_states).max(1)[0].detach()\n        target = rewards + (1 - dones) * self.gamma * next_q\n    \n        loss = F.mse_loss(curr_q.squeeze(), target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T05:08:55.037398Z","iopub.execute_input":"2025-03-06T05:08:55.037707Z","iopub.status.idle":"2025-03-06T05:08:55.073732Z","shell.execute_reply.started":"2025-03-06T05:08:55.037684Z","shell.execute_reply":"2025-03-06T05:08:55.072934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_multi_agent(players, episodes, max_steps):\n    env = FootballEnv()\n    agents = [DQNAgent(8, 8) for _ in players]\n    \n    for episode in range(episodes):\n        env.reset(players)\n        states = [env._get_state(p, players) for p in players]\n        total_rewards = [0.0 for _ in players]\n        losses = [[] for _ in players]\n        done = False\n        truncated = False\n        goal_team = None\n\n        for step in range(max_steps):\n            if done or truncated:\n                break\n\n            actions = [agents[i].act(states[i]) for i in range(len(players))]\n            experiences = []\n            \n            for i in range(len(players)):\n                next_state, reward, done, truncated, info = env.step(\n                    actions[i], players[i], players\n                )\n                experiences.append((states[i], actions[i], reward, next_state, done or truncated))\n                total_rewards[i] += reward\n                states[i] = next_state\n\n            if 'goal_team' in info and info['goal_team'] is not None:\n                goal_team = info['goal_team']\n                for i in range(len(players)):\n                    team_reward = players[i].goal_reward if players[i].team == goal_team else players[i].opp_scoring\n                    exp = list(experiences[i])\n                    exp[2] += team_reward\n                    experiences[i] = tuple(exp)\n                    total_rewards[i] += team_reward\n\n            for i in range(len(players)):\n                agents[i].remember(*experiences[i])\n                if len(agents[i].memory) >= agents[i].batch_size:\n                    loss = agents[i].replay()\n                    losses[i].append(loss)\n\n        for idx, agent in enumerate(agents):\n            agent.episode_count += 1\n            if agent.episode_count % agent.target_update_freq == 0:\n                agent.update_target_model()\n            agent.decay_epsilon()\n            agent.rewards_history.append(total_rewards[idx])\n\n        avg_losses = [np.mean(loss) if loss else 0.0 for loss in losses]\n        if episode %1000 == 0:\n            print(episode)\n        # print(f\"Episode {episode + 1}/{episodes}\")\n        # print(f\"Total Rewards: {total_rewards}\")\n        # print(f\"Average Losses: {avg_losses}\")\n        # print(f\"Epsilon: {agents[0].epsilon:.3f}\")\n        # env.render(players)\n\n    return agents\n\nenv = FootballEnv()\nplayers = [Player('F', i%2, env) for i in range(2)]\ntrained_agents = train_multi_agent(players, episodes=10000, max_steps=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T05:09:05.923367Z","iopub.execute_input":"2025-03-06T05:09:05.923672Z","iopub.status.idle":"2025-03-06T05:30:40.800639Z","shell.execute_reply.started":"2025-03-06T05:09:05.923651Z","shell.execute_reply":"2025-03-06T05:30:40.799925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluvating the agents\ndef evaluate_multi_agent(agents, env, players, episodes=1, max_steps=1000, render=True):\n    episode_rewards = []\n    \n    for episode in range(episodes):\n        env.reset(players)\n        total_rewards = [0.0 for _ in players]\n        done = False\n        truncated = False\n        \n        for step in range(max_steps):\n            if done or truncated:\n                break\n            \n            if render:\n                env.render(players)\n                time.sleep(0.1)\n            \n            # Get states for all players\n            states = [env._get_state(player, players) for player in players]\n            \n            # Process actions for all players in random order\n            order = np.random.permutation(len(players))\n            for idx in order:\n                agent = agents[idx]\n                player = players[idx]\n                \n                action = agent.act(states[idx], evaluate=True)  # No exploration\n                next_state, reward, done, truncated, _ = env.step(action, player, players)\n                # print(idx,action,player.has_ball)\n                \n                total_rewards[idx] += reward\n                \n                if done or truncated:\n                    break\n\n        episode_rewards.append(total_rewards)\n        print(f\"Total Rewards: {total_rewards}\")\n    \n    return np.mean(episode_rewards, axis=0)\n\n\navg_rewards = evaluate_multi_agent(\n    trained_agents,\n    env,\n    players,\n    episodes=1,\n    render=True\n)\nprint(f\"Average rewards across evaluation episodes: {avg_rewards}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T05:32:07.451599Z","iopub.execute_input":"2025-03-06T05:32:07.451958Z","iopub.status.idle":"2025-03-06T05:32:26.926034Z","shell.execute_reply.started":"2025-03-06T05:32:07.451928Z","shell.execute_reply":"2025-03-06T05:32:26.924939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}