{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb2f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import random\n",
    "from gymnasium import Env, spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0381171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c989490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    points = []\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    sx = 1 if x0 < x1 else -1\n",
    "    sy = 1 if y0 < y1 else -1\n",
    "    err = dx - dy\n",
    "    \n",
    "    while True:\n",
    "        points.append((x0, y0))\n",
    "        if x0 == x1 and y0 == y1:\n",
    "            break\n",
    "        e2 = 2 * err\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x0 += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y0 += sy\n",
    "            \n",
    "    return points\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, field_height, field_width, pos=None):\n",
    "        self.field_height = field_height\n",
    "        self.field_width = field_width\n",
    "        self.action_space = spaces.Discrete(10)  # 8 moves + shoot + stay still\n",
    "        self.pos = pos\n",
    "        self.position = np.array(pos) if pos is not None else np.array([\n",
    "            np.random.randint(0, field_height),\n",
    "            np.random.randint(0, field_width)\n",
    "        ])\n",
    "        self.shoot_speed = 4\n",
    "        self.shoot_speed_decay = 1\n",
    "\n",
    "    def policy(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])\n",
    "\n",
    "    def reset_position(self):\n",
    "        if self.pos is None:\n",
    "            self.position = np.array([\n",
    "                np.random.randint(0, self.field_height),\n",
    "                np.random.randint(0, self.field_width)\n",
    "            ])\n",
    "        return self.position\n",
    "\n",
    "    def shoot_ball(self, current_position, goal_position):\n",
    "        # Convert positions to integers for Bresenham algorithm\n",
    "        start_x, start_y = int(current_position[0]), int(current_position[1])\n",
    "        target_x, target_y = int(goal_position[0]), int(goal_position[1])\n",
    "        \n",
    "        # Calculate trajectory points using Bresenham's algorithm\n",
    "        trajectory_points = bresenham_line(start_x, start_y, target_x, target_y)\n",
    "        \n",
    "        # Skip the first point (current position)\n",
    "        if len(trajectory_points) > 1:\n",
    "            trajectory_points = trajectory_points[1:]\n",
    "        \n",
    "        # Calculate direction vector (normalized)\n",
    "        direction = np.array([goal_position[0] - current_position[0], \n",
    "                             goal_position[1] - current_position[1]])\n",
    "        direction_norm = np.linalg.norm(direction)\n",
    "        \n",
    "        if direction_norm > 0:\n",
    "            direction = direction / direction_norm\n",
    "        else:\n",
    "            direction = np.array([0, 0])\n",
    "        \n",
    "        return trajectory_points, self.shoot_speed, direction\n",
    "    \n",
    "\n",
    "class FootballEnv(Env):\n",
    "    def __init__(self, field_height, field_width, agent):\n",
    "        super(FootballEnv, self).__init__()\n",
    "\n",
    "        self.field_height = field_height  # x-axis (rows)\n",
    "        self.field_width = field_width  # y-axis (columns)\n",
    "        self.goal_y = self.field_width - 1  # Goal at the far right (y-axis)\n",
    "\n",
    "        # Observation Space: (player_x, player_y, ball_x, ball_y)\n",
    "        self.observation_space = spaces.Discrete(field_height * field_width)\n",
    "\n",
    "        self.field = np.full((self.field_height, self.field_width), '.', dtype=str)\n",
    "        self.rewards = np.full((self.field_height, self.field_width), -1, dtype=np.float32)\n",
    "        \n",
    "        # Goal area has a higher reward\n",
    "        self.rewards[:, self.goal_y] = 10.0  \n",
    "\n",
    "        self.ball_holder = 0\n",
    "        self.agent = agent\n",
    "        self.ball_in_transit = False\n",
    "        self.ball_transit_speed = 0\n",
    "        self.ball_transit_direction = None\n",
    "        self.ball_trajectory = []\n",
    "        self.ball_trajectory_index = 0\n",
    "\n",
    "        self.agent.q_table = np.zeros((self.observation_space.n, agent.action_space.n))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agents_position= self.agent.reset_position()\n",
    "        # self.ball_pos = np.array([np.random.randint(0, self.field_height), np.random.randint(0, self.field_width)]  # (x, y) format\n",
    "        self.ball_pos = np.array([2, 0])  # Center of the field\n",
    "        self.ball_holder = 0\n",
    "        self.ball_in_transit = False\n",
    "        self.ball_transit_speed = 0\n",
    "        self.ball_transit_direction = None\n",
    "        self.ball_trajectory = []\n",
    "        self.ball_trajectory_index = 0\n",
    "        self.done = False\n",
    "\n",
    "        if self.ball_holder == 0:\n",
    "            if np.array_equal(self.agent.position, self.ball_pos):\n",
    "                self.ball_holder = 1\n",
    "                self.ball_pos = self.agent.position.copy()\n",
    "                self.ball_in_transit = False\n",
    "\n",
    "        return self._get_agent_position(), self.ball_pos, self.ball_holder\n",
    "\n",
    "    def _get_agent_position(self):\n",
    "        return self.agent.position[0] * self.field_width + self.agent.position[1]\n",
    "    \n",
    "    def closest_goal_cell(self, agent_pos):\n",
    "        # select one cell from the goal area\n",
    "        goal_cells = [(i, self.goal_y) for i in range(self.field_height)]\n",
    "        # return the closest goal cell to the agent\n",
    "        closest_cell = min(goal_cells, key=lambda cell: np.linalg.norm(np.array(agent_pos) - np.array(cell)))\n",
    "        return closest_cell\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0  # Initialize rewards\n",
    "\n",
    "        # Define movement mapping for 8 directions\n",
    "        move_map = {\n",
    "            0: np.array([0, 1]),   # Right\n",
    "            1: np.array([1, 1]),   # Down-Right\n",
    "            2: np.array([1, 0]),   # Down\n",
    "            3: np.array([1, -1]),  # Down-Left\n",
    "            4: np.array([0, -1]),  # Left\n",
    "            5: np.array([-1, -1]), # Up-Left\n",
    "            6: np.array([-1, 0]),  # Up\n",
    "            7: np.array([-1, 1])   # Up-Right\n",
    "        }\n",
    "\n",
    "        # Get the action for the agent\n",
    "        if action in move_map:\n",
    "            move = move_map[action]\n",
    "            new_position = self.agent.position + move\n",
    "\n",
    "            # Check if the new position is within bounds\n",
    "            if (0 <= new_position[0] < self.field_height) and (0 <= new_position[1] < self.field_width):\n",
    "                self.agent.position = new_position\n",
    "                if self.ball_holder == 0:\n",
    "                    # Check if the agent is on the ball\n",
    "                    if np.array_equal(self.agent.position, self.ball_pos):\n",
    "                        self.ball_holder = 1\n",
    "                        self.ball_pos = self.agent.position.copy()  # Ball is now with the agent\n",
    "                if self.ball_holder == 1:\n",
    "                    self.ball_pos = self.agent.position.copy()  # Ball is with the agent\n",
    "        \n",
    "        elif action == 8:  # Shoot\n",
    "            # Check if the agent is holding the ball\n",
    "            if self.ball_holder == 1:\n",
    "                closest_shot = self.closest_goal_cell(self.agent.position)\n",
    "                print(f\"Agent shooting towards: {closest_shot}\")\n",
    "                # Calculate the trajectory of the ball\n",
    "                self.ball_trajectory, self.ball_transit_speed, self.ball_transit_direction = self.agent.shoot_ball(self.agent.position, closest_shot)\n",
    "                self.ball_in_transit = True\n",
    "                self.ball_holder = 0  # Ball is no longer held by the agent\n",
    "                self.ball_trajectory_index = 0  # Reset trajectory index\n",
    "        \n",
    "        elif action == 9:  # Stay still\n",
    "            pass  # Agent doesn't move\n",
    "        \n",
    "        # Handle ball movement if in transit\n",
    "        if self.ball_in_transit:\n",
    "            # First apply speed decay at the beginning of the step\n",
    "            if self.ball_transit_speed > 0:\n",
    "                self.ball_transit_speed -= self.agent.shoot_speed_decay\n",
    "                \n",
    "                # If speed has decayed to zero or below, stop the ball\n",
    "                if self.ball_transit_speed <= 0:\n",
    "                    self.ball_in_transit = False\n",
    "                    self.ball_transit_speed = 0\n",
    "                    print(\"Ball stopped due to speed decay\")\n",
    "            \n",
    "            # If ball is still in transit after speed decay\n",
    "            if self.ball_in_transit:\n",
    "                # Calculate how many steps to move based on current speed\n",
    "                steps_to_move = max(1, int(round(self.ball_transit_speed)))\n",
    "                \n",
    "                # Move the ball along its trajectory by the number of steps determined by speed\n",
    "                for _ in range(steps_to_move):\n",
    "                    if self.ball_trajectory_index < len(self.ball_trajectory):\n",
    "                        # Get the next position from the trajectory\n",
    "                        next_pos = self.ball_trajectory[self.ball_trajectory_index]\n",
    "                        self.ball_pos = np.array(next_pos)\n",
    "                        self.ball_trajectory_index += 1\n",
    "                        \n",
    "                        # Check if ball reached the goal\n",
    "                        if self.ball_pos[1] == self.goal_y:\n",
    "                            # Add goal reward (1000) + field reward (10) = 1010\n",
    "                            goal_reward = 1000\n",
    "                            field_reward = self.rewards[self.ball_pos[0], self.ball_pos[1]]\n",
    "                            total_reward = goal_reward + field_reward\n",
    "                            reward += total_reward\n",
    "                            print(f\"Goal! Reward: {total_reward}\")\n",
    "                            self.ball_holder = 0  # Ball is no longer held by the agent\n",
    "                            self.done = True  # End the episode on goal\n",
    "                            self.ball_in_transit = False\n",
    "                            break\n",
    "                    else:\n",
    "                        # Ball has completed its trajectory\n",
    "                        self.ball_in_transit = False\n",
    "                        self.ball_transit_speed = 0\n",
    "                        print(\"Ball reached end of trajectory\")\n",
    "                        break\n",
    "                        \n",
    "                # Check if any agent is at the ball's position after movement\n",
    "                if not self.ball_in_transit and np.array_equal(self.agent.position, self.ball_pos):\n",
    "                    self.ball_holder = 1  # Agent reclaims the ball\n",
    "        \n",
    "        # Only add field reward if we haven't already scored a goal (to avoid double counting)\n",
    "        if not self.done:\n",
    "            # Get the current state reward from the rewards matrix\n",
    "            state_reward = self.rewards[self.ball_pos[0], self.ball_pos[1]]\n",
    "            reward += state_reward\n",
    "        \n",
    "        # Return the new state, reward, and done flag\n",
    "        return self._get_agent_position(), self.ball_pos, self.ball_holder, reward, self.done, {}\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        field_copy = np.full((self.field_height, self.field_width), '.', dtype=str)\n",
    "        \n",
    "        # First, place the ball (if it's not held by an agent)\n",
    "        if self.ball_holder == 0:\n",
    "            field_copy[self.ball_pos[0], self.ball_pos[1]] = 'B'  # (x, y) indexing\n",
    "        \n",
    "        # render 'A' for agent\n",
    "        field_copy[self.agent.position[0], self.agent.position[1]] = 'A'\n",
    "        # render 'G' for goal area\n",
    "        field_copy[:, self.goal_y] = 'G'\n",
    "                \n",
    "        # If ball is in transit, show trajectory\n",
    "        if self.ball_in_transit:\n",
    "            for idx, (x, y) in enumerate(self.ball_trajectory):\n",
    "                if idx >= self.ball_trajectory_index:  # Only show remaining trajectory\n",
    "                    if 0 <= x < self.field_height and 0 <= y < self.field_width:\n",
    "                        if field_copy[x, y] == '.':  # Don't overwrite agents\n",
    "                            field_copy[x, y] = '*'\n",
    "\n",
    "        print(\"\\n\".join([\"\".join(row) for row in field_copy]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313f793",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m FootballEnv(field_height, field_width, agent)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save trained models\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# torch.save(agent.actor.state_dict(), \"football_actor.pth\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# torch.save(agent.critic.state_dict(), \"football_critic.pth\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 286\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, num_episodes)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    285\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mpolicy(env\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mposition, env\u001b[38;5;241m.\u001b[39mball_pos, env\u001b[38;5;241m.\u001b[39mball_holder)\n\u001b[1;32m--> 286\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# if episode % 100 == 0:\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m#     # env.render()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 258\u001b[0m, in \u001b[0;36mFootballEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mstore_experience(state, action, reward, next_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m--> 258\u001b[0m     critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m critic_loss\n\u001b[0;32m    260\u001b[0m     info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m actor_loss\n",
      "Cell \u001b[1;32mIn[6], line 101\u001b[0m, in \u001b[0;36mAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m--> 101\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(np\u001b[38;5;241m.\u001b[39marray([exp[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    103\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray([exp[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# field_height = 7\n",
    "# field_width = 10\n",
    "\n",
    "# # Create agent and environment\n",
    "# agent = Agent(field_height, field_width)\n",
    "# env = FootballEnv(field_height, field_width, agent)\n",
    "\n",
    "\n",
    "# # Save trained models\n",
    "# # torch.save(agent.actor.state_dict(), \"football_actor.pth\")\n",
    "# # torch.save(agent.critic.state_dict(), \"football_critic.pth\")\n",
    "\n",
    "# print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12a370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........G\n",
      "..........G\n",
      "A.........G\n",
      "..........G\n",
      "..........G\n",
      "\n",
      "Action: 0, State: 23, Ball Position: [2 1], Ball Holder: 1, Reward: -1.0\n",
      "..........G\n",
      "..........G\n",
      ".A........G\n",
      "..........G\n",
      "..........G\n",
      "\n",
      "Agent shooting towards: (2, 10)\n",
      "Action: 8, State: 23, Ball Position: [2 4], Ball Holder: 0, Reward: -1.0\n",
      "..........G\n",
      "..........G\n",
      ".A..B*****G\n",
      "..........G\n",
      "..........G\n",
      "\n",
      "Action: 9, State: 23, Ball Position: [2 6], Ball Holder: 0, Reward: -1.0\n",
      "..........G\n",
      "..........G\n",
      ".A....B***G\n",
      "..........G\n",
      "..........G\n",
      "\n",
      "Action: 9, State: 23, Ball Position: [2 7], Ball Holder: 0, Reward: -1.0\n",
      "..........G\n",
      "..........G\n",
      ".A.....B**G\n",
      "..........G\n",
      "..........G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(5, 11, pos=(2, 0))\n",
    "env = FootballEnv(5, 11, agent)\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "# take steps 0, 8 and three 9s and print reward at each step\n",
    "for action in [0, 8, 9, 9]:\n",
    "    state, ball_pos, ball_holder, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, State: {state}, Ball Position: {ball_pos}, Ball Holder: {ball_holder}, Reward: {reward}\")\n",
    "    env.render()\n",
    "    time.sleep(1)  # Pause for a second to visualize the rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e376f8",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
