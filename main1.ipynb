{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# model and env\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport gymnasium as gym\nfrom collections import deque\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FootballEnv(gym.Env):\n    def __init__(self, grid_rows=10, grid_cols=10):\n        super(FootballEnv, self).__init__()\n        self.grid_rows = grid_rows\n        self.grid_cols = grid_cols\n\n        self.action_space = gym.spaces.Discrete(10)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(7,), dtype=np.float32)\n        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n        self.layout[:, :] = \".\"\n        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n        self.layout[:, self.grid_cols//2] = \"M\"\n        self.layout[:, -1] = \"O\"\n        self.layout[:, 0] = \"O\"\n        self.layout[0, :] = \"O\"\n        self.layout[-1, :] = \"O\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n        self.episode_steps = 0\n\n\n    def _get_state(self,player,players):\n\n        same_team = []\n        other_team = []\n        for play in players:\n            if player.team == play.team:\n                same_team.extend([play.position[0]/self.grid_rows -1 ,play.position[1]/self.grid_cols -1])\n            else:\n                other_team.extend([play.position[0]/self.grid_rows -1 ,play.position[1]/self.grid_cols -1])\n        other_team = [-1*x for x in other_team]\n        # print(same_team,other_team)\n        return np.array([\n            *same_team,\n            *other_team,\n            float(player.has_ball),\n            self.grid_rows // 2 / (self.grid_rows - 1),\n            (self.grid_cols - 1) / (self.grid_cols - 1)\n        ], dtype=np.float32)\n\n\n    def reset(self, players, seed=None, options=None):\n        super().reset(seed=seed)\n\n        occupied_positions = set()\n        for player in players:\n            while True:\n                if player.team == 0:\n                    player_row = random.randint(self.grid_rows // 2 - 3, self.grid_rows // 2 + 3)\n                    player_col = random.randint(1, self.grid_cols // 4)\n                else:\n                    player_row = random.randint(self.grid_rows // 2 - 3, self.grid_rows // 2 + 3)\n                    player_col = random.randint(3 * self.grid_cols // 4, self.grid_cols - 2)\n\n                if (player_row, player_col) not in occupied_positions:\n                    occupied_positions.add((player_row, player_col))\n                    player.position = (player_row, player_col)\n                    break\n\n        while True:\n            ball_row = random.randint(1, self.grid_rows - 2)  # Avoid boundary walls\n            ball_col = random.randint(self.grid_cols // 4, 3 * self.grid_cols // 4)  # Avoid goalposts\n\n\n            if (ball_row, ball_col) not in occupied_positions:\n                self.ball_pos = (ball_row, ball_col)\n                break\n        self.episode_steps = 0\n        return {}, {}\n\n\n    def step(self, action,player,players):\n        self.episode_steps += 1\n        reward = player.step_penalty\n        done = False\n\n        if action < 8:  # Movement actions\n            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n            new_pos = (player.position[0] + dx, player.position[1] + dy)\n\n            if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols and self.layout[new_pos[0],new_pos[1]] != \"O\":\n                player.prev_position = player.position\n                player.position = new_pos\n                if player.has_ball:\n                    self.ball_pos = new_pos\n            else:\n                reward -= 20  # Massive penalty\n                done = True  # End episode if the player goes out\n                # return self._get_state(), reward, done, True, {}\n                return self._get_state(player,players), reward, done, True, {}\n\n        # elif action == 8 and player.has_ball:  # Long shot\n        #     goal_y_center = self.grid_rows // 2\n        #     # Better aim toward goal\n        #     target_y = min(max(goal_y_center + random.randint(-2, 2), 0), self.grid_rows-1)\n        #     new_ball_col = min(self.ball_pos[1] + 10, self.grid_cols - 1)\n        #     self.ball_pos = (target_y, new_ball_col)\n        #     self.has_ball = False\n\n        # elif action == 9 and self.has_ball:  # Short pass\n        #     new_ball_col = min(self.ball_pos[1] + 5, self.grid_cols - 1)\n        #     self.ball_pos = (self.ball_pos[0], new_ball_col)\n        #     self.has_ball = False\n\n        # Check if player gets the ball\n        player.prev_ball_statues = player.has_ball\n        player.has_ball = player.position == self.ball_pos\n        \n        # Reward shaping\n        if player.has_ball:\n            reward += player.ball_possession_bonus\n\n        if player.has_ball:\n            reward += 0.001\n        \n        # Distance-based rewards\n        dist_to_ball = np.sqrt((player.position[0] - self.ball_pos[0])**2 + \n                              (player.position[1] - self.ball_pos[1])**2)\n        if dist_to_ball < 5 and not player.has_ball:\n            reward += player.near_ball_bonus\n        \n        if player.has_ball:\n            # Calculate distance to goal\n            dist_to_goal = self.grid_cols - 1 - player.position[1]\n            if dist_to_goal < 10:\n                reward += player.near_goal_bonus * (1 - dist_to_goal/10.0)\n        \n        # Goal reward\n        if self.layout[self.ball_pos[0], self.ball_pos[1]] == 'G':\n            reward += player.goal_reward\n            done = True\n\n        truncated = self.episode_steps >= 3000\n        return self._get_state(player,players), reward, done, truncated, {}\n\n    def render(self,players):\n        grid = np.full((self.grid_rows, self.grid_cols), '-')\n        \n        # Draw field elements\n        for i in range(self.grid_rows):\n            for j in range(self.grid_cols):\n                if self.layout[i,j] == 'O':\n                    grid[i,j] = '#'\n                elif self.layout[i,j] == 'G':\n                    grid[i,j] = '|'\n                elif self.layout[i,j] == 'M':\n                    grid[i,j] = '.'\n\n        grid[self.ball_pos[0], self.ball_pos[1]] = 'B'\n        \n        # Draw player and ball\n        for player in players:\n            if player.team == 0:\n                grid[player.position[0], player.position[1]] = 'P'\n            else:\n                grid[player.position[0], player.position[1]] = 'Q'\n\n        \n        \n        # Print the grid\n        print('-' * (self.grid_cols + 2))\n        for row in grid:\n            print('|' + ''.join(row) + '|')\n        print('-' * (self.grid_cols + 2))\n\n\nclass Player():\n    def __init__(self,role,team,env):\n        self.role = role\n        self.team = team\n        self.position = [random.randint(env.grid_rows//2-3, env.grid_rows//2+3),random.randint(1, env.grid_cols//4)]\n        self.prev_position = []\n        self.prev_ball_statues = False\n        self.has_ball = False\n        self.goal_reward = 52\n        self.step_penalty = -0.001\n        self.ball_possession_bonus = 0.008\n        self.near_ball_bonus = 0.00001\n        self.near_goal_bonus = 0.00002\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, action_size)\n        \n        # Initialize weights with better defaults\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = 0.99  # Discount factor\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.0003\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 256\n        self.target_update_freq = 5  # Update target network every N episodes\n        \n        self.device = device\n        self.model = DQN(state_size, action_size).to(self.device)\n        self.target_model = DQN(state_size, action_size).to(self.device)\n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        \n        self.rewards_history = []\n        self.episode_count = 0\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state, evaluate=False):\n        if not evaluate and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n            \n        minibatch = random.sample(self.memory, self.batch_size)\n    \n        states = torch.FloatTensor(np.array([experience[0] for experience in minibatch])).to(self.device)\n        actions = torch.LongTensor([experience[1] for experience in minibatch]).to(self.device)\n        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(self.device)\n        next_states = torch.FloatTensor(np.array([experience[3] for experience in minibatch])).to(self.device)\n        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(self.device)\n    \n        curr_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n    \n        with torch.no_grad():\n            next_q_values = self.target_model(next_states).max(1)[0]\n    \n        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n    \n        loss = F.mse_loss(curr_q_values.squeeze(), target_q_values)\n    \n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  \n        self.optimizer.step()\n    \n        return loss.item()\n\n    def update_target_model(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    def decay_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay       \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T06:14:51.692591Z","iopub.execute_input":"2025-03-05T06:14:51.692941Z","iopub.status.idle":"2025-03-05T06:14:51.726525Z","shell.execute_reply.started":"2025-03-05T06:14:51.692912Z","shell.execute_reply":"2025-03-05T06:14:51.725790Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Training the agents\ndef train_multi_agent(players,episodes,max_steps):\n    agents = [DQNAgent(7,10) for player in players]\n    for episode in range(episodes):\n        # print(f\"Episode {episode}\")\n        env.reset(players)\n        total_rewards = [0.0 for _ in players]\n        losses = [[] for _ in players]\n        truncated = False\n        done = False\n        for step in range(max_steps):\n            if not done and not truncated:\n                states = [env._get_state(player, players) for player in players]\n                for i in range(len(players)):\n                    action = agents[i].act(states[i])\n                    next_state,reward,done,truncated,_=env.step(action,players[i],players)\n                    agents[i].remember(states[i],action,reward,next_state,done or truncated)\n                    if len(agents[i].memory) >= agents[i].batch_size:\n                        loss = agents[i].replay()\n                        losses[i].append(loss)\n                    total_rewards[i] += reward\n                    if done or truncated:\n                        break\n        for idx, agent in enumerate(agents):\n            agent.episode_count += 1\n            if agent.episode_count % agent.target_update_freq == 0:\n                agent.update_target_model()\n            agent.decay_epsilon()\n            agent.rewards_history.append(total_rewards[idx])\n\n        avg_losses = [np.mean(loss) if loss else 0.0 for loss in losses]\n        # print(f\"Episode {episode + 1}/{episodes}\")\n        # print(f\"Total Rewards: {total_rewards}\")\n        # print(f\"Average Losses: {avg_losses}\")\n        # print(f\"Epsilon: {agents[0].epsilon:.3f}\")\n        # env.render(players)\n    return agents\n\nenv = FootballEnv()\nplayers = [Player('F',i%2,env) for i in range(2)]\nagents = train_multi_agent(players,100,1000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T06:14:54.564701Z","iopub.execute_input":"2025-03-05T06:14:54.565126Z","iopub.status.idle":"2025-03-05T06:14:57.853176Z","shell.execute_reply.started":"2025-03-05T06:14:54.565090Z","shell.execute_reply":"2025-03-05T06:14:57.852295Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Evaluvating the agents\ndef evaluate_multi_agent(agents, env, players, episodes=1, max_steps=1000, render=True):\n    episode_rewards = []\n    \n    for episode in range(episodes):\n        env.reset(players)\n        total_rewards = [0.0 for _ in players]\n        done = False\n        truncated = False\n        \n        for step in range(max_steps):\n            if done or truncated:\n                break\n            \n            if render:\n                env.render(players)\n                time.sleep(0.1)\n            \n            # Get states for all players\n            states = [env._get_state(player, players) for player in players]\n            \n            # Process actions for all players in random order\n            order = np.random.permutation(len(players))\n            for idx in order:\n                agent = agents[idx]\n                player = players[idx]\n                \n                action = agent.act(states[idx], evaluate=True)  # No exploration\n                next_state, reward, done, truncated, _ = env.step(action, player, players)\n                \n                total_rewards[idx] += reward\n                \n                if done or truncated:\n                    break\n\n        episode_rewards.append(total_rewards)\n        print(f\"Evaluation Episode {episode+1}:\")\n        print(f\"Total Rewards: {total_rewards}\")\n        print(\"-\" * 50)\n    \n    return np.mean(episode_rewards, axis=0)\n\n\navg_rewards = evaluate_multi_agent(\n    agents,\n    env,\n    players,\n    episodes=100,\n    render=False\n)\nprint(f\"Average rewards across evaluation episodes: {avg_rewards}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T06:15:36.944576Z","iopub.execute_input":"2025-03-05T06:15:36.944894Z","iopub.status.idle":"2025-03-05T06:15:46.864577Z","shell.execute_reply.started":"2025-03-05T06:15:36.944866Z","shell.execute_reply":"2025-03-05T06:15:46.863231Z"}},"outputs":[{"name":"stdout","text":"Evaluation Episode 1:\nTotal Rewards: [8.013988000000133, -0.9900100000000197]\n--------------------------------------------------\nEvaluation Episode 2:\nTotal Rewards: [8.011987999999922, -0.9900200000000198]\n--------------------------------------------------\nEvaluation Episode 3:\nTotal Rewards: [-0.9900000000000198, -0.9900100000000197]\n--------------------------------------------------\nEvaluation Episode 4:\nTotal Rewards: [-0.9900100000000197, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 5:\nTotal Rewards: [-0.9900000000000198, -1.0000000000000007]\n--------------------------------------------------\nEvaluation Episode 6:\nTotal Rewards: [-0.9900000000000198, -1.0000000000000007]\n--------------------------------------------------\nEvaluation Episode 7:\nTotal Rewards: [-0.9900100000000197, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 8:\nTotal Rewards: [-0.9900000000000198, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 9:\nTotal Rewards: [-0.9900000000000198, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 10:\nTotal Rewards: [-1.0000000000000007, -1.0000000000000007]\n--------------------------------------------------\nEvaluation Episode 11:\nTotal Rewards: [-1.0000000000000007, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 12:\nTotal Rewards: [-0.9900000000000198, -1.0000000000000007]\n--------------------------------------------------\nEvaluation Episode 13:\nTotal Rewards: [-0.9900000000000198, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 14:\nTotal Rewards: [-0.9900000000000198, -0.9900000000000198]\n--------------------------------------------------\nEvaluation Episode 15:\nTotal Rewards: [-1.0000000000000007, -0.9900000000000198]\n--------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0469022e4a23>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m avg_rewards = evaluate_multi_agent(\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-0469022e4a23>\u001b[0m in \u001b[0;36mevaluate_multi_agent\u001b[0;34m(agents, env, players, episodes, max_steps, render)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ba9b659d8245>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, evaluate)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ba9b659d8245>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}