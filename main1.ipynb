{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FootballEnv(gym.Env):\n",
    "    def __init__(self, grid_rows=10, grid_cols=10):\n",
    "        super(FootballEnv, self).__init__()\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(10)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(7,), dtype=np.float32)\n",
    "        self.layout = np.zeros((grid_rows, grid_cols), dtype=str)\n",
    "        self.layout[:, :] = \".\"\n",
    "        self.layout[self.grid_rows//2, self.grid_cols//2] = \"C\"\n",
    "        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, -6:-1] = \"D\"\n",
    "        self.layout[self.grid_rows//2-8 : self.grid_rows//2+9, 0:5] = \"d\"\n",
    "        self.layout[:, self.grid_cols//2] = \"M\"\n",
    "        self.layout[:, -1] = \"O\"\n",
    "        self.layout[:, 0] = \"O\"\n",
    "        self.layout[0, :] = \"O\"\n",
    "        self.layout[-1, :] = \"O\"\n",
    "        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, -1] = \"G\"\n",
    "        self.layout[self.grid_rows//2-4 : self.grid_rows//2+5, 0] = \"g\"\n",
    "        self.ball_pos = (self.grid_rows//2, self.grid_cols//2)\n",
    "        self.episode_steps = 0\n",
    "\n",
    "\n",
    "    def _get_state(self,player,players):\n",
    "\n",
    "        same_team = []\n",
    "        other_team = []\n",
    "        for play in players:\n",
    "            if player.team == play.team:\n",
    "                same_team.extend([play.position[0]/self.grid_rows -1 ,play.position[1]/self.grid_cols -1])\n",
    "            else:\n",
    "                other_team.extend([play.position[0]/self.grid_rows -1 ,play.position[1]/self.grid_cols -1])\n",
    "\n",
    "        print(same_team,other_team)\n",
    "        return np.array([\n",
    "            *same_team,\n",
    "            *other_team,\n",
    "            float(player.has_ball),\n",
    "            self.grid_rows // 2 / (self.grid_rows - 1),\n",
    "            (self.grid_cols - 1) / (self.grid_cols - 1)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def reset(self, players, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        occupied_positions = set()\n",
    "        for player in players:\n",
    "            while True:\n",
    "                if player.team == 0:\n",
    "                    player_row = random.randint(self.grid_rows // 2 - 3, self.grid_rows // 2 + 3)\n",
    "                    player_col = random.randint(1, self.grid_cols // 4)\n",
    "                else:\n",
    "                    player_row = random.randint(self.grid_rows // 2 - 3, self.grid_rows // 2 + 3)\n",
    "                    player_col = random.randint(3 * self.grid_cols // 4, self.grid_cols - 2)\n",
    "\n",
    "                if (player_row, player_col) not in occupied_positions:\n",
    "                    occupied_positions.add((player_row, player_col))\n",
    "                    player.position = (player_row, player_col)\n",
    "                    break\n",
    "\n",
    "        while True:\n",
    "            ball_row = random.randint(self.grid_rows // 2, self.grid_rows // 2)\n",
    "            ball_col = random.randint(2, self.grid_cols // 4)\n",
    "\n",
    "            if (ball_row, ball_col) not in occupied_positions:\n",
    "                self.ball_pos = (ball_row, ball_col)\n",
    "                break\n",
    "        self.episode_steps = 0\n",
    "        return {}, {}\n",
    "\n",
    "\n",
    "    def step(self, action,player,players):\n",
    "        self.episode_steps += 1\n",
    "        reward = player.step_penalty\n",
    "        done = False\n",
    "\n",
    "        if action < 8:  # Movement actions\n",
    "            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0), (-1, -1), (-1, 1), (1, -1), (1, 1)][action]\n",
    "            new_pos = (player.position[0] + dx, player.position[1] + dy)\n",
    "\n",
    "            if 0 <= new_pos[0] < self.grid_rows and 0 <= new_pos[1] < self.grid_cols and self.layout[new_pos[0],new_pos[1]] != \"O\":\n",
    "                player.prev_position = player.position\n",
    "                player.position = new_pos\n",
    "                if player.has_ball:\n",
    "                    self.ball_pos = new_pos\n",
    "            else:\n",
    "                reward -= 20  # Massive penalty\n",
    "                done = True  # End episode if the player goes out\n",
    "                # return self._get_state(), reward, done, True, {}\n",
    "                return self._get_state(player,players), reward, done, truncated, {}\n",
    "\n",
    "        # elif action == 8 and player.has_ball:  # Long shot\n",
    "        #     goal_y_center = self.grid_rows // 2\n",
    "        #     # Better aim toward goal\n",
    "        #     target_y = min(max(goal_y_center + random.randint(-2, 2), 0), self.grid_rows-1)\n",
    "        #     new_ball_col = min(self.ball_pos[1] + 10, self.grid_cols - 1)\n",
    "        #     self.ball_pos = (target_y, new_ball_col)\n",
    "        #     self.has_ball = False\n",
    "\n",
    "        # elif action == 9 and self.has_ball:  # Short pass\n",
    "        #     new_ball_col = min(self.ball_pos[1] + 5, self.grid_cols - 1)\n",
    "        #     self.ball_pos = (self.ball_pos[0], new_ball_col)\n",
    "        #     self.has_ball = False\n",
    "\n",
    "        # Check if player gets the ball\n",
    "        player.prev_ball_statues = player.has_ball\n",
    "        player.has_ball = player.position == self.ball_pos\n",
    "        \n",
    "        # Reward shaping\n",
    "        if player.has_ball:\n",
    "            reward += player.ball_possession_bonus\n",
    "\n",
    "        if player.has_ball:\n",
    "            reward += 0.001\n",
    "        \n",
    "        # Distance-based rewards\n",
    "        dist_to_ball = np.sqrt((player.position[0] - self.ball_pos[0])**2 + \n",
    "                              (player.position[1] - self.ball_pos[1])**2)\n",
    "        if dist_to_ball < 5 and not player.has_ball:\n",
    "            reward += player.near_ball_bonus\n",
    "        \n",
    "        # Reward for moving toward goal with ball\n",
    "        if player.has_ball:\n",
    "            # Calculate distance to goal\n",
    "            dist_to_goal = self.grid_cols - 1 - player.position[1]\n",
    "            if dist_to_goal < 10:\n",
    "                reward += self.near_goal_bonus * (1 - dist_to_goal/10.0)\n",
    "        \n",
    "        # Goal reward\n",
    "        if self.layout[self.ball_pos[0], self.ball_pos[1]] == 'G':\n",
    "            reward += self.goal_reward\n",
    "            done = True\n",
    "\n",
    "        truncated = self.episode_steps >= 3000  # Shorter episodes\n",
    "        # return self._get_state(), reward, done, truncated, {}\n",
    "        return self._get_state(player,players), reward, done, truncated, {}\n",
    "\n",
    "    def render(self,players):\n",
    "        grid = np.full((self.grid_rows, self.grid_cols), '-')\n",
    "        \n",
    "        # Draw field elements\n",
    "        for i in range(self.grid_rows):\n",
    "            for j in range(self.grid_cols):\n",
    "                if self.layout[i,j] == 'O':\n",
    "                    grid[i,j] = '#'\n",
    "                elif self.layout[i,j] == 'G':\n",
    "                    grid[i,j] = '|'\n",
    "                elif self.layout[i,j] == 'M':\n",
    "                    grid[i,j] = '.'\n",
    "        \n",
    "        # Draw player and ball\n",
    "        for player in players:\n",
    "            if player.team == 0:\n",
    "                grid[player.position[0], player.position[1]] = 'P'\n",
    "            else:\n",
    "                grid[player.position[0], player.position[1]] = 'Q'\n",
    "\n",
    "            if player.has_ball:\n",
    "                continue\n",
    "        else:\n",
    "            grid[self.ball_pos[0], self.ball_pos[1]] = 'B'\n",
    "        \n",
    "        # Print the grid\n",
    "        print('-' * (self.grid_cols + 2))\n",
    "        for row in grid:\n",
    "            print('|' + ''.join(row) + '|')\n",
    "        print('-' * (self.grid_cols + 2))\n",
    "\n",
    "    # This train is for testing purpose of the code\n",
    "    # def train(self,episodes,players):\n",
    "    #     for episode in range(episodes):\n",
    "    #         truncated = False\n",
    "    #         done = False\n",
    "    #         if not done and not truncated:\n",
    "    #             for player in players:\n",
    "    #                 action = random.randint(0,8)\n",
    "    #                 _,reward,done,truncated,_=self.step(action,player)\n",
    "    #                 print(reward)\n",
    "    #                 print(action)\n",
    "    #         self.render(players)\n",
    "\n",
    "class Player():\n",
    "    def __init__(self,role,team,env):\n",
    "        self.role = role\n",
    "        self.team = team\n",
    "        self.position = [random.randint(env.grid_rows//2-3, env.grid_rows//2+3),random.randint(1, env.grid_cols//4)]\n",
    "        self.prev_position = []\n",
    "        self.prev_ball_statues = False\n",
    "        self.has_ball = False\n",
    "        self.goal_reward = 52\n",
    "        self.step_penalty = -0.001\n",
    "        self.ball_possession_bonus = 0.008\n",
    "        self.near_ball_bonus = 0.00001\n",
    "        self.near_goal_bonus = 0.00002\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "        \n",
    "        # Initialize weights with better defaults\n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0003\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 256\n",
    "        self.target_update_freq = 5  # Update target network every N episodes\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.rewards_history = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if not evaluate and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "            \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([experience[0] for experience in minibatch]).to(self.device)\n",
    "        actions = torch.LongTensor([experience[1] for experience in minibatch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([experience[3] for experience in minibatch]).to(self.device)\n",
    "        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(self.device)\n",
    "        \n",
    "        curr_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        \n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        loss = F.mse_loss(curr_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'episode_count': self.episode_count,\n",
    "            'rewards_history': self.rewards_history\n",
    "        }, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.target_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.episode_count = checkpoint['episode_count']\n",
    "        self.rewards_history = checkpoint['rewards_history']\n",
    "        print(f\"Model loaded from {filepath}\")                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_agent(players,episodes,max_steps):\n",
    "    agents = [DQNAgent(7,10) for player in players]\n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode {episode}\")\n",
    "        env.reset(players)\n",
    "        total_rewards = [0.0 for _ in players]\n",
    "        losses = [[] for _ in players]\n",
    "        truncated = False\n",
    "        done = False\n",
    "        for step in range(max_steps):\n",
    "            if not done and not truncated:\n",
    "                states = [env._get_state(player, players) for player in players]\n",
    "                for i in range(len(players)):\n",
    "                    action = agents[i].act(states[i])\n",
    "                    next_state,reward,done,truncated,_=env.step(action,players[i],players)\n",
    "                    agents[i].remember(states[i],action,reward,next_state,done or truncated)\n",
    "                    if len(agents[i].memory) >= agents[i].batch_size:\n",
    "                        loss = agents[i].replay()\n",
    "                        losses[i].append(loss)\n",
    "                    total_rewards[i] += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "        for idx, agent in enumerate(agents):\n",
    "            agent.episode_count += 1\n",
    "            if agent.episode_count % agent.target_update_freq == 0:\n",
    "                agent.update_target_model()\n",
    "            agent.decay_epsilon()\n",
    "            agent.rewards_history.append(total_rewards[idx])\n",
    "\n",
    "        avg_losses = [np.mean(loss) if loss else 0.0 for loss in losses]\n",
    "        print(f\"Episode {episode + 1}/{episodes}\")\n",
    "        print(f\"Total Rewards: {total_rewards}\")\n",
    "        print(f\"Average Losses: {avg_losses}\")\n",
    "        print(f\"Epsilon: {agents[0].epsilon:.3f}\")\n",
    "        env.render(players)\n",
    "    return agents\n",
    "\n",
    "env = FootballEnv()\n",
    "players = [Player('F',i%2,env) for i in range(2)]\n",
    "agents = train_multi_agent(players,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multi_agent(agents, env, players, episodes=1, max_steps=1000, render=True):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        env.reset(players)\n",
    "        total_rewards = [0.0 for _ in players]\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if done or truncated:\n",
    "                break\n",
    "            \n",
    "            if render:\n",
    "                env.render(players)\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            # Get states for all players\n",
    "            states = [env._get_state(player, players) for player in players]\n",
    "            \n",
    "            # Process actions for all players in random order\n",
    "            order = np.random.permutation(len(players))\n",
    "            for idx in order:\n",
    "                agent = agents[idx]\n",
    "                player = players[idx]\n",
    "                \n",
    "                action = agent.act(states[idx], evaluate=True)  # No exploration\n",
    "                next_state, reward, done, truncated, _ = env.step(action, player, players)\n",
    "                \n",
    "                total_rewards[idx] += reward\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "        episode_rewards.append(total_rewards)\n",
    "        print(f\"Evaluation Episode {episode+1}:\")\n",
    "        print(f\"Total Rewards: {total_rewards}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return np.mean(episode_rewards, axis=0)\n",
    "\n",
    "\n",
    "avg_rewards = evaluate_multi_agent(\n",
    "    agents,\n",
    "    env,\n",
    "    players,\n",
    "    episodes=5,\n",
    "    render=True\n",
    ")\n",
    "print(f\"Average rewards across evaluation episodes: {avg_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
